{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL-Models-HIDS.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NganTran-0017/HIDS/blob/main/DL_Models_HIDS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Mzc_frRBqSW"
      },
      "source": [
        ""
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baLX852RsM7a"
      },
      "source": [
        "#@title Specify parameters before running\n",
        "\n",
        "\n",
        "SZ =  1#@param {type:\"number\"}         # Indicate a fraction number to sample train set when it's too big. Located in Data Partition\n",
        "\n",
        "SEQ_WINDOW =  10#@param {type:\"integer\"} # Indicate the window length to parse the sequence into. Used in Data Parsing section\n",
        "\n",
        "BATCH_SZ =  512#@param {type:\"integer\"} # Indicate the window length to parse the sequence into. Used in Data Parsing section\n",
        "\n",
        "EPOCHS =  2#@param {type:\"integer\"} # Indicate the window length to parse the sequence into. Used in Data Parsing section\n",
        "\n",
        "# Indicate to clean data or not. Used in Data Cleaning section\n",
        "CLEAN = True #@param {type:\"boolean\"}\n"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDDU0UkRplxK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e56f120-c3a3-497f-afe9-625a64b5ae5d"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, accuracy_score, roc_curve, auc, recall_score, precision_score,plot_confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"popular\")\n",
        "\n",
        "## Tokenizing syscall sequences into n-grams of 6\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import ngrams"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4SwCY4NVnCL"
      },
      "source": [
        "#**Processing data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQhzBhHanZuz"
      },
      "source": [
        "Use the given datasets in our GitHub to load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHtiWDffY_so",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8e94b19-9515-4352-9aa6-c96c866ae2b0"
      },
      "source": [
        "  ## Uncomment each line to load Normal data\n",
        "\n",
        "# Synthetic sendmail csv_file = ['bounce-1.int', 'bounce.int', 'bounce-2.int', 'plus.int', 'queue.int', 'sendmail.daemon.int', 'sendmail.log.int']; DATA = 'Synthetic Sendmail'; DATA_I='Synthetic_Sendmail'\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/1.Synthetic%20Sendmail/normal-data/bounce-1.int'\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/1.Synthetic%20Sendmail/normal-data/bounce.int'\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/1.Synthetic%20Sendmail/normal-data/bounce-2.int'\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/1.Synthetic%20Sendmail/normal-data/plus.int'\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/1.Synthetic%20Sendmail/normal-data/queue.int'\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/1.Synthetic%20Sendmail/normal-data/sendmail.daemon.int'\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/1.Synthetic%20Sendmail/normal-data/sendmail.log.int'\n",
        "\n",
        "# LIVE LPR csv_file = ['lpr-normal-10.txt', 'lpr-normal-11.txt']; DATA = 'Live Lpr'; DATA_I='Live-Lpr' \n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/4.%20Live%20lpr/Normal/real/lpr-normal-11.txt'\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/4.%20Live%20lpr/Normal/real/lpr-normal-10.txt'\n",
        "\n",
        "# MIT live lpr csv_file = [ 'mit-lpr-mar.txt']; DATA = 'MIT Live Lpr'; DATA_I='MIT-Lpr' \n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/MIT/mit-lpr-mar.txt'\n",
        "\n",
        "# LOGIN and PS csv_file = [ 'login-normal.txt', 'ps-normal.txt']; DATA = 'Login and Ps'; DATA_I =\"Login-and-Ps\"\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/7.Login_and_ps/normal/login-normal.txt'\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/7.Login_and_ps/normal/ps-normal.txt'\n",
        "\n",
        "# INETD csv_file = [ 'inetd-live-unm.int']; DATA = 'Inetd';DATA_I =\"Inetd\" \n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/8.Inetd/inetd-live-unm.int'\n",
        "\n",
        "# STIDE \n",
        "csv_file = [ 'stide-normal-500k.txt']; DATA = 'Stide';DATA_I ='Stide' \n",
        "!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/10.Stide/stide-normal-500k.txt'\n",
        "\n",
        "# Live Named  ==> Best result csv_file = [ 'normal-named-live-2k.txt']; DATA = 'Live Named';DATA_I ='Live-Named' \n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/6.Live_named/normal-named-live-2k.txt'\n",
        "\n",
        "# Xlock csv_file = [ 'normal-xlock.txt']; DATA = 'Xlock';DATA_I='Xlock' \n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/5.xlock/normal-xlock.txt'\n",
        "\n",
        "# Synthetic Ftp csv_file = [ 'nonself1.int','nonself2.int']; DATA = 'Synthetic Ftp'; DATA_I='Synthetic-Ftp'\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/2.Synthetic%20Ftp/nonself1.int'\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/2.Synthetic%20Ftp/nonself2.int'\n",
        "\n",
        "# Synthetic lpr csv_file = ['syn.int']; DATA = 'Synthetic Lpr';DATA_I='Synthetic-Lpr'\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/3.Synthetic-lpr/syn.int'\n",
        "\n",
        "# Concat a list of files into normal df\n",
        "list_of_dataframes = []\n",
        "for filename in csv_file:\n",
        "    list_of_dataframes.append(pd.read_csv(filename, sep='\\t', header=None, engine='python'))\n",
        "df = pd.concat(list_of_dataframes)\n",
        "\n",
        "# Check number of columns, if > 2, then drop the excess\n",
        "if len(df.columns) > 2:\n",
        "    df=df.drop(labels=None, axis=1, columns = [2,3])\n",
        "df =df.rename(columns= {0:\"PID\", 1:\"Syscall\"})\n",
        "\n",
        "print('Normal data size:', df.shape)\n",
        "\n",
        "\n",
        "  ## Uncomment each line to load Intrusion data:\n",
        "\n",
        "# Synthetic sendmail csv_file = ['sm-10763.int', 'fwd-loops-1.int', 'fwd-loops-2.int', 'fwd-loops-3.int', 'fwd-loops-4.int', 'fwd-loops-5.int','sm-280.int', 'sm-314.int','sm-10801.int', 'sm-10814.int']\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/1.Synthetic%20Sendmail/Intrusion-trace-data/fwd-loops-1.int'\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/1.Synthetic%20Sendmail/Intrusion-trace-data/fwd-loops-2.int'\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/1.Synthetic%20Sendmail/Intrusion-trace-data/fwd-loops-3.int'\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/1.Synthetic%20Sendmail/Intrusion-trace-data/fwd-loops-4.int'\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/1.Synthetic%20Sendmail/Intrusion-trace-data/fwd-loops-5.int'\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/1.Synthetic%20Sendmail/Intrusion-trace-data/sm-10763.int'\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/1.Synthetic%20Sendmail/Intrusion-trace-data/sm-280.int'\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/1.Synthetic%20Sendmail/Intrusion-trace-data/sm-314.int'\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/1.Synthetic%20Sendmail/Intrusion-trace-data/sm-10801.int'\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/1.Synthetic%20Sendmail/Intrusion-trace-data/sm-10814.int'\n",
        "\n",
        "# LIVE LPR csv_file =['exploit-unm.int'] \n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/4.%20Live%20lpr/Intrusion/exploit-unm.int'\n",
        "\n",
        "# MIT live lpr csv_file = [ 'exploit-ai.int'] \n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/MIT/exploit-ai.int'\n",
        "\n",
        "# LOGIN and PS csv_file = [ 'login-homegrown.int','ps-homegrown.int','login-recovered.int','ps-recovered.int']\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/7.Login_and_ps/intrusion/ps-recovered.int'\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/7.Login_and_ps/intrusion/ps-homegrown.int'\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/7.Login_and_ps/intrusion/login-recovered.int'\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/7.Login_and_ps/intrusion/login-homegrown.int'\n",
        "\n",
        "# INETD csv_file = [ 'inetd-intrusion.int'] \n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/8.Inetd/intrusion/inetd-intrusion.int'\n",
        "\n",
        "# STIDE \n",
        "csv_file = [ 'stide-intrusion'] \n",
        "!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/10.Stide/intrusion/stide-intrusion'\n",
        "\n",
        "# Live Named  ==> Best Result csv_file = [ 'exploit-1.int','exploit-2.int'] \n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/6.Live_named/intrusion/exploit-1.int'\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/6.Live_named/intrusion/exploit-2.int'\n",
        "\n",
        "# Xlock csv_file = [ 'nonself.cs.unm.edu-07.24.97-xlock-2822_new.log.int', 'nonself.cs.unm.edu-07.25.97-xlock-2691_new.log.int']\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/5.xlock/intrusion/nonself.cs.unm.edu-07.25.97-xlock-2691_new.log.int'\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/5.xlock/intrusion/nonself.cs.unm.edu-07.24.97-xlock-2822_new.log.int'\n",
        "\n",
        "# Synthetic Ftp csv_file = [ 'exploit2.int']\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/2.Synthetic%20Ftp/intrusion/exploit2.int'\n",
        "\n",
        "# Synthetic Lpr csv_file = [ 'exploit-unm.int']\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/3.Synthetic-lpr/intrusion/exploit-unm.int'\n",
        "\n",
        "list_of_dataframes = []\n",
        "for filename in csv_file:\n",
        "    list_of_dataframes.append(pd.read_csv(filename, sep=' ', header=None, engine='python'))\n",
        "intrusiondf = pd.concat(list_of_dataframes)\n",
        "\n",
        "if len(intrusiondf.columns) > 2:\n",
        "    intrusiondf = intrusiondf.drop(labels=None, axis=1, columns = [2,3])\n",
        "intrusiondf = intrusiondf.rename(columns= {0:\"PID\", 1:\"Syscall\"})\n",
        "\n",
        "print('intrusion data size: ', intrusiondf.shape)\n",
        "\n",
        "print('Normal:',df.head(3))\n",
        "#print(df['PID'].value_counts())\n",
        "print('Intrusion:',intrusiondf.head())"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-10 02:17:54--  https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/10.Stide/stide-normal-500k.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4048353 (3.9M) [text/plain]\n",
            "Saving to: ‘stide-normal-500k.txt.1’\n",
            "\n",
            "stide-normal-500k.t 100%[===================>]   3.86M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2021-11-10 02:17:54 (54.3 MB/s) - ‘stide-normal-500k.txt.1’ saved [4048353/4048353]\n",
            "\n",
            "Normal data size: (499722, 2)\n",
            "--2021-11-10 02:17:57--  https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/10.Stide/intrusion/stide-intrusion\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1427163 (1.4M) [text/plain]\n",
            "Saving to: ‘stide-intrusion.1’\n",
            "\n",
            "stide-intrusion.1   100%[===================>]   1.36M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2021-11-10 02:17:57 (26.0 MB/s) - ‘stide-intrusion.1’ saved [1427163/1427163]\n",
            "\n",
            "intrusion data size:  (205935, 2)\n",
            "Normal:    PID  Syscall\n",
            "0  405       90\n",
            "1  405      125\n",
            "2  405      125\n",
            "Intrusion:    PID  Syscall\n",
            "0  930       90\n",
            "1  930      125\n",
            "2  930      125\n",
            "3  930      106\n",
            "4  930        5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09eozmwq9CFh"
      },
      "source": [
        "**Change to covert all syscall of 1 PID into a data record. Pasrse each data record to a length of 10 or 15, clean frequent records.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "6OgG7OGdQ0Oi",
        "outputId": "febc639b-0ed2-4c3f-82dc-b5b01d34605d"
      },
      "source": [
        "print('Number of unique PID in normal data:', len(df['PID'].value_counts()))\n",
        "df"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique PID in normal data: 279\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PID</th>\n",
              "      <th>Syscall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>405</td>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>405</td>\n",
              "      <td>125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>405</td>\n",
              "      <td>125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>405</td>\n",
              "      <td>106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>405</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499717</th>\n",
              "      <td>20010</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499718</th>\n",
              "      <td>20010</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499719</th>\n",
              "      <td>20010</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499720</th>\n",
              "      <td>20010</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499721</th>\n",
              "      <td>20010</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>499722 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          PID  Syscall\n",
              "0         405       90\n",
              "1         405      125\n",
              "2         405      125\n",
              "3         405      106\n",
              "4         405        5\n",
              "...       ...      ...\n",
              "499717  20010        4\n",
              "499718  20010        4\n",
              "499719  20010        4\n",
              "499720  20010        4\n",
              "499721  20010        4\n",
              "\n",
              "[499722 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "D3SbpSdF7YDQ",
        "outputId": "c3ab17e1-ddf7-4ce9-f7c1-ee6a1c60b212"
      },
      "source": [
        "print('Number of unique PID in intrusion data:', len(intrusiondf['PID'].value_counts()))\n",
        "intrusiondf"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique PID in intrusion data: 105\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PID</th>\n",
              "      <th>Syscall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>930</td>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>930</td>\n",
              "      <td>125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>930</td>\n",
              "      <td>125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>930</td>\n",
              "      <td>106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>930</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>205930</th>\n",
              "      <td>1202</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>205931</th>\n",
              "      <td>1202</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>205932</th>\n",
              "      <td>1202</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>205933</th>\n",
              "      <td>1202</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>205934</th>\n",
              "      <td>1202</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>205935 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         PID  Syscall\n",
              "0        930       90\n",
              "1        930      125\n",
              "2        930      125\n",
              "3        930      106\n",
              "4        930        5\n",
              "...      ...      ...\n",
              "205930  1202       45\n",
              "205931  1202       45\n",
              "205932  1202       45\n",
              "205933  1202       45\n",
              "205934  1202       45\n",
              "\n",
              "[205935 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvHyIKtq9BCG"
      },
      "source": [
        "**Create syscall sequence per pid**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXOlXVgyOHmA",
        "outputId": "d6e6c346-2d80-4256-fb1b-761a8bce31c6"
      },
      "source": [
        "# This function groups data by PID, so the sequences appear by PID instead of by order, in case it was interrupted by other PID\n",
        "## It returns a dict with PID as key and syscall seq as item\n",
        "def group_syscalls_by_pid (data):\n",
        "  seq_per_pid = {}\n",
        "  for p in data['PID'].unique():\n",
        "    filt = data['PID'] == p\n",
        "    seq = data.loc[filt]['Syscall'].values.astype(str)\n",
        "    seq_per_pid[p] = ' '.join(seq)\n",
        "  return seq_per_pid\n",
        "\n",
        "# Group normal df by PID and drop PID column\n",
        "normal_seq_per_pid = group_syscalls_by_pid(df)\n",
        "print('Number of unique PID in normal:', len(normal_seq_per_pid))\n",
        "#print('Normal PIDs and its sequences: ',normal_seq_per_pid)\n",
        "\n",
        "# Do the same thing to intrusion PID\n",
        "intrusion_seq_per_pid = group_syscalls_by_pid(intrusiondf)\n",
        "print('Number of unique PID in intrusion:', len(intrusion_seq_per_pid))\n",
        "#print('Intrusion PIDs and its sequences: ', intrusion_seq_per_pid)"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique PID in normal: 279\n",
            "Number of unique PID in intrusion: 105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3M7Z4L6modm",
        "outputId": "ae351b29-fbd6-4085-e274-f2e282c4bb4b"
      },
      "source": [
        "# Drop a sequence if its total len is less than 3\n",
        "def remove_small_seq(pid_seq_dict):\n",
        "  removed_pid = []\n",
        "  for pid in pid_seq_dict:\n",
        "    seq_list = pid_seq_dict[pid].split()\n",
        "    if len(seq_list) < 3:\n",
        "      print('Remove PID %d which only has %d syscals in its sequence: %s' % (pid, len(seq_list), pid_seq_dict[pid]))\n",
        "      removed_pid.append(pid)\n",
        "\n",
        "  [pid_seq_dict.pop(pid) for pid in removed_pid]\n",
        "  return pid_seq_dict\n",
        "\n",
        "# Clean small intrusion sequences\n",
        "print('Clean small normal seq: \\nNum PID in Normal before:', len(normal_seq_per_pid))\n",
        "normal_seq_per_pid = remove_small_seq(normal_seq_per_pid)\n",
        "print('Num PID in Normal after:', len(normal_seq_per_pid))\n",
        "\n",
        "# Clean small intrusion sequences\n",
        "print('\\n\\nClean small intrusion seq: \\nNum PID in Intrusion before:', len(intrusion_seq_per_pid))\n",
        "intrusion_seq_per_pid = remove_small_seq(intrusion_seq_per_pid)\n",
        "print('Num PID in Intrusion after:', len(intrusion_seq_per_pid))"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean small normal seq: \n",
            "Num PID in Normal before: 279\n",
            "Num PID in Normal after: 279\n",
            "\n",
            "\n",
            "Clean small intrusion seq: \n",
            "Num PID in Intrusion before: 105\n",
            "Num PID in Intrusion after: 105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKXCVvBRF9fB"
      },
      "source": [
        "## **Data Parsing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "ph244VM5_86n",
        "outputId": "5af19643-1e54-4f51-e80d-dcb6ec9973b4"
      },
      "source": [
        "## Parse an entire Syscall seq per PID into smaller sequences of size 15\n",
        "def parse_seq(seq_per_pid):\n",
        "  sequences = pd.DataFrame()\n",
        "  for p in seq_per_pid:\n",
        "    token = word_tokenize(seq_per_pid[p])  # Tokenize the string of sequence\n",
        "\n",
        "    # Parse the sequence into length of 15\n",
        "    sequences=sequences.append(list(nltk.ngrams(token, SEQ_WINDOW, pad_right=True, right_pad_symbol=-1)))\n",
        "    #print('PID %d - seq len: %d'% (p, len(sequences)))\n",
        "  return sequences\n",
        "\n",
        "normal = parse_seq(normal_seq_per_pid)\n",
        "\n",
        "normal"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>90</td>\n",
              "      <td>125</td>\n",
              "      <td>125</td>\n",
              "      <td>106</td>\n",
              "      <td>5</td>\n",
              "      <td>90</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>125</td>\n",
              "      <td>125</td>\n",
              "      <td>106</td>\n",
              "      <td>5</td>\n",
              "      <td>90</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>90</td>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>125</td>\n",
              "      <td>106</td>\n",
              "      <td>5</td>\n",
              "      <td>90</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>90</td>\n",
              "      <td>90</td>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>106</td>\n",
              "      <td>5</td>\n",
              "      <td>90</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>90</td>\n",
              "      <td>90</td>\n",
              "      <td>90</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>90</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>90</td>\n",
              "      <td>90</td>\n",
              "      <td>90</td>\n",
              "      <td>6</td>\n",
              "      <td>125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1156</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1157</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1158</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1159</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1160</th>\n",
              "      <td>4</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>499722 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0    1    2    3   4   5   6   7   8    9\n",
              "0      90  125  125  106   5  90   6   5   3   90\n",
              "1     125  125  106    5  90   6   5   3  90   90\n",
              "2     125  106    5   90   6   5   3  90  90   90\n",
              "3     106    5   90    6   5   3  90  90  90    6\n",
              "4       5   90    6    5   3  90  90  90   6  125\n",
              "...   ...  ...  ...  ...  ..  ..  ..  ..  ..  ...\n",
              "1156    4    4    4    4   4  -1  -1  -1  -1   -1\n",
              "1157    4    4    4    4  -1  -1  -1  -1  -1   -1\n",
              "1158    4    4    4   -1  -1  -1  -1  -1  -1   -1\n",
              "1159    4    4   -1   -1  -1  -1  -1  -1  -1   -1\n",
              "1160    4   -1   -1   -1  -1  -1  -1  -1  -1   -1\n",
              "\n",
              "[499722 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "oyAnefsOQiIU",
        "outputId": "67ccb247-2d4b-42aa-8b63-51b2e1b00e59"
      },
      "source": [
        "print('Parsing Intrusion')\n",
        "intrusion = parse_seq(intrusion_seq_per_pid)\n",
        "intrusion"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing Intrusion\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>90</td>\n",
              "      <td>125</td>\n",
              "      <td>125</td>\n",
              "      <td>106</td>\n",
              "      <td>5</td>\n",
              "      <td>90</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>125</td>\n",
              "      <td>125</td>\n",
              "      <td>106</td>\n",
              "      <td>5</td>\n",
              "      <td>90</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>90</td>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>125</td>\n",
              "      <td>106</td>\n",
              "      <td>5</td>\n",
              "      <td>90</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>90</td>\n",
              "      <td>90</td>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>106</td>\n",
              "      <td>5</td>\n",
              "      <td>90</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>90</td>\n",
              "      <td>90</td>\n",
              "      <td>90</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>90</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>90</td>\n",
              "      <td>90</td>\n",
              "      <td>90</td>\n",
              "      <td>6</td>\n",
              "      <td>125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>45</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>205935 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      0    1    2    3   4   5   6   7   8    9\n",
              "0    90  125  125  106   5  90   6   5   3   90\n",
              "1   125  125  106    5  90   6   5   3  90   90\n",
              "2   125  106    5   90   6   5   3  90  90   90\n",
              "3   106    5   90    6   5   3  90  90  90    6\n",
              "4     5   90    6    5   3  90  90  90   6  125\n",
              "..  ...  ...  ...  ...  ..  ..  ..  ..  ..  ...\n",
              "21   45   45   45   45  45  -1  -1  -1  -1   -1\n",
              "22   45   45   45   45  -1  -1  -1  -1  -1   -1\n",
              "23   45   45   45   -1  -1  -1  -1  -1  -1   -1\n",
              "24   45   45   -1   -1  -1  -1  -1  -1  -1   -1\n",
              "25   45   -1   -1   -1  -1  -1  -1  -1  -1   -1\n",
              "\n",
              "[205935 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WjkWBLbqW3u"
      },
      "source": [
        "Start tokenizing system calls into 6-grams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "376udEA5GdPB"
      },
      "source": [
        " ## **Data Cleaning**\n",
        " Remove rows that exist in both normal and intrusion df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdvbPMAOl-Cn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9e9509d-96c2-4c5b-c2bc-2d7c0f3781dd"
      },
      "source": [
        "## Get % of duplicates in both datasets\n",
        "\n",
        "def clean_data (normal, intrusion):\n",
        "  # Convert normal df to set, and intrusion df to set\n",
        "  normal_list = normal.values.tolist()\n",
        "  intrusion_list = intrusion.values.tolist()\n",
        "  normal_set = set(tuple(i) for i in normal_list)\n",
        "  intrusion_set = set(tuple(i) for i in intrusion_list)\n",
        "  print('List sz vs. Set sz of normal sequences: %d vs. %d'% (len(normal_list),len(normal_set)) )\n",
        "  print('List sz vs. Set sz of intrusion sequences: %d vs. %d'% (len(intrusion_list),len(intrusion_set)) )\n",
        "\n",
        "  # Only remove intrusion sequences that exist in normal set because these sequences are just general actions which should not be labelled intrusion\n",
        "  c_intrusion = intrusion_set - normal_set\n",
        "  #c_normal = normal_set - intrusion_set\n",
        "  if len(c_intrusion) == 0 and len(c_normal) == 0:\n",
        "    print(DATA+' No Duplication!')\n",
        "  if len(c_intrusion) > 0:\n",
        "    intrusion = pd.DataFrame(c_intrusion)\n",
        "  else:\n",
        "    intrusion = pd.DataFrame(intrusion_set)\n",
        "\n",
        "  #if len(c_normal) > 0:\n",
        "  #  normal = pd.DataFrame(c_normal)\n",
        "  #else:\n",
        "  normal = pd.DataFrame(normal_set)\n",
        "\n",
        "  print('After cleaning: \\nNormal sz:', len(normal), ' CLEAN Intrusion sz:', len(c_intrusion) )\n",
        "  return normal, intrusion\n",
        "\n",
        "if CLEAN:\n",
        "  normal, intrusion = clean_data(normal, intrusion)"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List sz vs. Set sz of normal sequences: 499722 vs. 373\n",
            "List sz vs. Set sz of intrusion sequences: 205935 vs. 315\n",
            "After cleaning: \n",
            "Normal sz: 373  CLEAN Intrusion sz: 113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "MuylikQOw7da",
        "outputId": "f9041b2d-84ad-4eb8-c720-9ea380d6b0b1"
      },
      "source": [
        "normal"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>19</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>5</td>\n",
              "      <td>108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>19</td>\n",
              "      <td>6</td>\n",
              "      <td>91</td>\n",
              "      <td>108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>45</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>108</td>\n",
              "      <td>90</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>368</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>108</td>\n",
              "      <td>90</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>369</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>5</td>\n",
              "      <td>108</td>\n",
              "      <td>90</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>370</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>371</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>372</th>\n",
              "      <td>91</td>\n",
              "      <td>108</td>\n",
              "      <td>90</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>373 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      0    1   2   3   4    5    6    7   8    9\n",
              "0    45   45  45  45  45   45   45   45  19    6\n",
              "1     4    4   4   4   4    4   45   45   5  108\n",
              "2    45   45  45  45  45   45   19    6  91  108\n",
              "3     4   45   4   3   5  108   90    4   4    4\n",
              "4    45   45  45  45  45   45   45   45  45   45\n",
              "..   ..  ...  ..  ..  ..  ...  ...  ...  ..  ...\n",
              "368   4    4   4   4   4    4  108   90   3    4\n",
              "369   4    4   4   4  45   45    5  108  90    3\n",
              "370   4    4   4   4   4    4    4   -1  -1   -1\n",
              "371   4    4   4   4   4    4   45   45  45   45\n",
              "372  91  108  90   4   4    4    4    4   4    4\n",
              "\n",
              "[373 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_b5U4P_HrSEm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "2b6086c0-1fed-4ed3-ca2d-9d88f28d6e8a"
      },
      "source": [
        "intrusion"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>19</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>5</td>\n",
              "      <td>108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>19</td>\n",
              "      <td>6</td>\n",
              "      <td>91</td>\n",
              "      <td>108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>90</td>\n",
              "      <td>3</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>19</td>\n",
              "      <td>6</td>\n",
              "      <td>91</td>\n",
              "      <td>108</td>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>19</td>\n",
              "      <td>6</td>\n",
              "      <td>91</td>\n",
              "      <td>108</td>\n",
              "      <td>90</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>45</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>310</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>108</td>\n",
              "      <td>90</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>311</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>5</td>\n",
              "      <td>108</td>\n",
              "      <td>90</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>312</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>119</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>313</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>119</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>314</th>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>3</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>315 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      0   1   2    3   4    5    6    7    8    9\n",
              "0    45  45  45   45  45   45   45   45   19    6\n",
              "1     4   4   4    4   4    4   45   45    5  108\n",
              "2    45  45  45   45  45   45   19    6   91  108\n",
              "3    90   3  45   45  45   19    6   91  108   90\n",
              "4    19   6  91  108  90    3    4    4   45    4\n",
              "..   ..  ..  ..  ...  ..  ...  ...  ...  ...  ...\n",
              "310   4   4   4    4   4    4  108   90    3    4\n",
              "311   4   4   4    4  45   45    5  108   90    3\n",
              "312   4   4   4    4   4  119    4    4    4    4\n",
              "313   4   4   4    4   4    4    4    4  119    4\n",
              "314  45  45  45    3  45   45   45   45   45    3\n",
              "\n",
              "[315 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GMYBPNarqFf"
      },
      "source": [
        "## **Histogram of Processed Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CB8QSEMJ4h0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "outputId": "16a15f00-dc44-4ed7-d3e3-02178658ab55"
      },
      "source": [
        "# After Cleaning\n",
        "plt.figure(figsize=(20,6))\n",
        "plt.hist(normal[0], label='Normal', alpha=0.6, density=True)\n",
        "plt.hist(intrusion[0], label='Intrusion', color='tomato', alpha=0.6, density=True)\n",
        "plt.legend()\n",
        "plt.ylabel('Proportions')\n",
        "plt.xlabel('Syscall num')\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "#plt.xticks(np.arange(0,200,10))\n",
        "plt.title('An Overlaid Histogram of Syscall Proportions in Normal and Intrusion Data from ' + DATA,y=1.02, fontsize=15)\n",
        "plt.show()"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABJUAAAGMCAYAAABj1ZMqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzde5gcVZn48e9LAoxyUy5qIEjiTxTQwKADrEYUXMGwKFHkDgrqbhYFQcFV2HVBUVZF7m52JSsXcWEDgkhWo4gKrCiXBBnJhouGECXgggQERAIE3t8fpybpNHPpmkxnJsn38zz9zFTVqeq3q09XV719zqnITCRJkiRJkqQ61hruACRJkiRJkrTqMakkSZIkSZKk2kwqSZIkSZIkqTaTSpIkSZIkSarNpJIkSZIkSZJqM6kkSZIkSZKk2kwqSRp2EXFfRGREvLYN294kIs6KiAUR8UxEPBgRF0TEVkP9XC3Ecn1EXFFznd2qffPGAcqdHhELBijz+Yh4pI9lF0XE7IbpI6rnXb/FOF9Xbf9lrZRfU0TE5Ii4KyKe7e/9iYg3RsT3IuIPEfF09ZmYPtD7Phya61ErdTQixlVleh5PRsTsiDhg5US94vqq43U/K+3W/FlewW0tqF7bwU3z16/mHzEUz9NuQ7lPViCG91T7bFw/ZVo63vey3s4R8fkVDLG2qn6cvpKfr+cY0vN9PjMiPhgRta9p2vG91eoxf7hExHsj4hcR8aeIeCIi5kbEN3qOXxGxTrVPOpvW6zmGv2eA7R8dEdnO1yBp5DGpJGlYRcRbgHHV5MH9FB3MtjcHbgX2A74K7AmcAOwMzI6INwzl87XJr4C3APeu5Of9QfW8f2mx/OuAkwGTSpWIGAVcDPwaeCfw/j7KvRa4GdgQOBrYG/gKsCmw/UoJduX5NKVefQD4LXDZQBcpI0hfdbzuZ6XdvggcMcTb/MeIiCHepobOzpS6ubK9Hzh3JT/npZTP2zuBY4AHgPOBmRGxds1tDen3VqvH/OFSJYdnAHMo51sHAN8CdmXZPliHsk86m1b/A2W/37hSgpW0Shk93AFIWuMdDDwF/G/1/xeHcNv/RjlR2j4zH+iZGRHfA2YD/wnsOITP16uIeElmPj2YdTPzCUrCYaXKzD8Cf1zZz1tHdZG7bmYuHu5Y+jCGkii6NDP7OxH/MPAMsFdmPlPN+xlw3mp4IX9PZt4MEBE/Ad4EfAz4fnPBkfL+9sTR1/KR9lnJzKFOQF8P7AZMBr43lBseKe/xmmKo93dm3j4U26npDz3HkMoVEXE58CPgH4EvDENMPVo65q/IOcEKOhqYmZlHNsz7EXDaQN811XfTSj8XkbRqsKWSpGFT/ap3AOWXswuAbSNih6YyPV1LJkTEtRHxVETcHRH7DrDtccA+wDmNCSVYmqg5FeiMiLdX5e+LiK/1sp3vRMSNDdMbR8S0iHgoIhZHxC8jYpemdTIijouIsyPij5RfBXuLcZuqi9P9EfGXqhn6Jxub8ffWHSIiXhYRl0bEn6N0l/qn/vbFYPTWpSciToyIedXrfigifhQRr4qI3YD/ror1dGVc0LBeZ0T8tHqNj0XEJRHxyqbne3VE/DCWdf06IiKuiIjrG8p8PiIeiYi3RcQsYDGwf0SsFxH/GhH3VM9xX0RMjYgNm54jI+JTEXFGRCyqtvXpatnhETG/6hJwQUR0tLCPDoiIOVG6YdwfEadGxOie/QfcXxW9unruz/exqZcBf2pIKC2VmVlt7+PV+71cF6uG+rFDNb1PRNxWfU4ei4hbIuIdDeVHVe/jb6q4F0bERQ3L964+Zw9H6Rpxc0TsOdC+GIzMfAHopmqp2Nf7Wy3rc19Xy3vq604R8fOqHv0mIl7UUiBK94zfVtuaFxGfalreVxy91vE+PiubRsS3qnr2lyhdX7uanmdBlG6rn6reh8eiHA9e1lBm7arM72NZd5+rImKdvvZr9N2VtdYxtMFtlAvPAY8zg923DfN3idIt8umIuDEixkfEK6J0Df1zlG5F72za5oeqso9W+/C65n3dioh4S0TMiHJMfSoiuiPi0KYyLe3LKD5ffY6ejIiLKcmG2qrnOzYi/iUi/lhtc2pErNsTE/D1hrIZ1XGzn/3da5fNaOrOVq338yjHgieqfbJ/X+Wrea1+VgdbH18kM68FvkNJUPc8T7/fr9HP91ZEjInyPTA/lh1LvjTA5+4I+jjmRx/nBFHvOHFCVTcfj/IdFhHxN9XrerL6jLx8gF31MuD/+tiHPV3Wnqz+XthQn8ZFL93fImLdKN+9f6o+f2cBL2otFi2cN0latZlUkjScdgdeCUwHrgCeo+8ucJdSkk/vp3SbmR4RY/vZ9q5A0Pcv6z3z3179vZzqArZHdcK9dxUf1Un8T4B3Af8AvI/SQuEnEfGqpu3/A+VXyw9Smuj3ZgvgHuDjwN8A/0H5lfWz/bwugAuBvYBPAVMo3foOGmCdxtc1uvlB2Vf9rfMhyq/AZwLvppy8zwPWo3TR+3RVdF9KE/n3V+ttRmnp8FLgEOATwDuAa3tO0CMiKO/ttsBHgOMo+6y3k86XUprrfxOYROne+FJgFOWidy/gnyldD77Ty/rHA+tT6tmlwNci4jRKd6Fjqtd4KPDJAfbHnsBl1WufTLmo+zTwr1WRH1T7ApZ1+fpmH5v7FfCaiDgnIrbro8yl1Wvcr2n+h4FfZeavI+L/UT5HPwPeW72O7wMbN5Q/j1LHLgfeQ9kfL21YPp5yofVBShe1XwI/jIiJfcS1osax/EXOi97fFvZ1o8uAqyn7fg7wnWhIVEfE31Xrz6Dso+8AZ0TECU3baY7jDvqo4334HuVz8mngQMr51nXx4nHjDgD+mvI5/izlPfmXhuUnUt7Hfwb2oNTLxyl1oa66x9BGXwK6ImJSXwVWYN/e2jB/GnAW5fP5auDbwH9RutzsS+nq9J2IaKyz4yhdjvanHGPuB34eEa9p8bX12Ar4BfDRKv4rKRfWvX0nDbQvjwFOql7PfsDTwGk142l0PLA5cBjwNeDvgWOrZT8Azqj+f0v1+HjDun3t735FScp/H5hPORbsR3k/+uwqVvOzuiL1sTfXAq+MZWNWDfT92uf3FqXr8aOU76JJlH3+4er19GWgY35v5wStHicOonRx/DClHh1H+S7+IuXYcCTle/XL/cTX85oPjpL83byPMj1J2y+xrD79oY+yXwH+torjUMpn6PjGAjXPmyStqjLThw8fPoblQRkH4TFgnWr6+8ACIBrKHAEk8JGGeZsAS4Aj+9n2CdV6G/VT5k/Av1f/71iV/6uG5QdXz/PKavqjwLPA1g1lRlPGO/paw7ykXOg3P9/1wBV9xBLVtv4RmN8wf7dqe2+spt9QTR/YUGZ9ygnwggH29+erdft6zO5lv69fTf8rcGU/235PVX5c0/yvVPt5w4Z5u1RlD66m966md2ooswUlyXh9L/FPHuB1jgYmVmVf3fS+XNcwvRblZPmxpvguB24Z4DlubtxWNe8zwPPA2Gp6XPWc72kh3ssa3odFlIu3rqZy/wnc0PS+/xk4upreD1jUz/NsU23/mBY/n2tVsV0DXND0PjzSVx3tY1s9+2KfapsbV/srG+Lv9f1tcV/31Nd/bIr/bmB6w/QDwIVN2/o3SqKmY4A4+qrjPc/d81mZVE2/o6HMepQLqfMa5i2gHDtGN8w7G/i/hunvA2e08n41rHMRvX+Wax1DG2I8vfr/euDnDXUvgSOGaN/2zG/cZx+v5p3UMG+7at5eA9TZu5vWW26ftLAPe47H5wE/q7MvKQm/B6m+WxrKXdtb/WkqsxtNn6Vq+n+ayn0PuLlh+miqxiZ97Nfm/b1cne3j/e6qymzQSv0YxGd1hepjL8veXW13l37ez+bv114/072sP5qSsFxMdb7SR7lx9HLMp5dzAuodJ+YBoxrm3Vrtr/EN804DHhrgdWxJaR3a810zn5KcelVDmeU+2329tuo9exr4bNPn7+7GukiL500+fPhYtR+2VJI0LKpWKvsCV2Xms9Xs6ZRfut7Syyo/7vknMxcBDwMr8qvmcrKMDfEbyq+FPQ6kXMQ/VE2/i9IV5L6GFj4AN1BOwBvNHOg5I6IjIr4QEfMoY+o8R+mWN76xu0CTnaq/VzfE/mfKBUsrHq+20fx40Zg2TbqBv6ni3TlK18VW7Az8OEuXw554b6GcKL+tmrUT5UJ6VkOZByj7ulkCP2yeGeXuP7dHxJ8p+7Gny+Lrmor+tOE5XgDuA25rjI9yAr9FXy+oeu1v4sUtoS6jnFT3Vn/7lJlLMvNAYAfKr863UVqw3BQRezcUPR/YtaEFxgGUk/NLq+k5wEZVd4o9I2K9pqfavfp7UT+vbWy1/gOUi5bnKC3hmvfjYF1dbXMR5ZfwM4F/b1i+3Ps7iH191dINlff3akodhHK82LyPbW0ITOgrjpp2Bh7OzBsaYnmK8hl7W1PZ6zJzScP0ncArYtmAw93AERHxmYjYvmrVN1gregz9EvC2aOhO2WAo9u2zwM8bpudVf3/Wy7yln8+I2DZKl8CHKMmL54DXU7PORsTLI+LciPhdtY3nKC3IettOf/tyS0qLlKub1vlunXj6er7KnbT+3g22Lt9LSVpfGuWOZv0OZj2Iz+pQf6cv99kY5Pdrz7oRpavcnRHxdLXuJZSx1V49yPiazwnqHCeuz8znG6bnUX5Euq9p3mbRTxe9zLwfeDPlXOYMyo9RnwLuGEQrsQlAB8ufi7zAi+t9nfMmSasok0qShstelGb0M6OMEfQyyi/hz9B7F7g/NU0/Szmh6UvPOEpb9bYwIjYCNmooB+Xkd//qhHJDyi+J0xuWbwr8FcsuOHoeH6ZcSDR6iIF9ldLsfRqlef5OlAs36Pu1vQp4Ml880OrDLTwfwJLMnN38oFzk9+cCyq+8BwC3AA9FGWNioOTSGHrfFw+xrFvWq+h9oOPe5j3WkIQEIMq4ORcDN1G6wPwVy7oxNO/H3upR3bq1KWXciObX1TO9MYOQmXdk5pcyc0/KRfEfWFYfoHw+5rPszl4fBq7OzEer9e+hdDl5DeUC5pEoY29tVpXfBHiqKYG2VJSxRmYAb6V03dmdUid/SP/7o45PVdvchtJK4vimi6Xm97fuvm7+HDxMqYM0/G1lWy+qZzWM6SWOnudpjre3utc4MPiXgKmUVju/Bu6PiGMZnLr1fDmZ+RPKZ7+3sZWGYt8+WV2UNsYHDXE3rNcBEBEbUJITW1K6BO1KqV+/pn6dvYjyQ8LXKInUnSjHvd6209++7OnS01tdHKwVee8GVZcz8zFKl8u1Ka03/xgRP+inW2Hdz+oK1cde9CQae55vMN+vPT4JnE5JUk+mJICOanHdvjTvlxU9TvR17OgzqQSQmc9n5k8z89OZ2UVp4bUxTd3WWtBqPa9z3iRpFeXd3yQNl57EUW/j3uwfEZ9sutis6+cs625zRy/L96n+/k/DvMsoLUXeRhlbZi2W/3X5Ucpd4z7GizUPspwtxLg/8PXMXDrWRlPLlN78H7BBRHQ0JZZe0cLzDVp1sXcWcFZEbEkZP+FUYCHwjX5W/UMfsb2SZS2R/g/YrJcym1G6GywXSi/l9qd0V1s6jkgfrSmGyiOUk+Lm19Uz+PijK/oEmbkgIr5Dw9gomZkRcQEwJSL+k1JP92pa7wfAD6qk6d6U7lRfp4zJsQhYLyI27COx9FpKN9C9MvNHPTMj4iUr+noazKuSmH1pfn/r7utXsHyC9BUsGw/kDw3zBtpWK5/fvvRX52vVjeozfhJwUkRsTRk75eyIuKfxPVqJTqUkHndumr+y9m2zt1Bat+yRmXf3zKzqf8uiDMz/HuCozPxGw/zB/PjaM0ZY875o6zG6H73t757janMCYrmBnrPcZW1SdQx4F6Vl4aWUJEGzth8XB7AnpcXrgmp6MN+vPfandFVfmkDtZ7y7VjW/D0N2nFgRmfnjiPg1JdFfR2M9b4y3+TXVOW+StIqypZKkla7qlvNeygCsuzc9jqOcVL2zzw20oDqxnAF8MiLGNC6LMgD354DuzPyfhnXmAv9L+bX6QOAnVbP8Hj+lXHj/vpfWPr3e4W0AL6HhpKpq9TPQgNs9XcQmN72ePQbx/IOSmfdn5lcoze17TrSXa0HQ4Bbg3VWLAgAiYifK+Aw9XdRmAa+KiJ0bymxBaabfiuX2Y+XQ3goOhSrZeRtNA7tTWnG9QGkx1bKI6Otic2te/Ov2RZSL6PMprex67faYmY9n5qWUX9p73qOebkQf6uP5epJHjXVyK8r4VMNiEPt66eDZVUJgMssGJl5IGeumt209QR93aWzQVx1vdgulC1vPTQCoBpbem2V1vrbM/C2l5cUzLHtPV6rM/G9KK6DPNS1a0X07WL3V2bdS3VGwhnUp58SN29mAZT8+1HE/5YJ7ctP8Qd/drAXPwtLkWCsWVn+37ZkR5W5cvd6hLjOfrt77C+ij7g31cbGOiNiDMqZcY1faVr5f+/pMr4zvlLYcJ/rT23dNVWfGsuy7ptXj3BxKcrLxXGQtXlzvh/q8SdIIZEslScNhMuWONOdU4+ssFRG/oHSvOJjWxwnqy8cpLZZujogvU8ah2IoycOjG9H6SfxnlrjobAX/XtOxiSkuB66PcRnk+pUvRzpRfSM+qGd+1wFHVmA+PUprXr9vfCpk5NyJmAP9eddH7A+WOKn+p+dy1RMR5VYw3U8Zl2p2S9Oi5k8491d+/j4jpwF+qE8YzKb9QXhMRX6UMAvoVygnpldU6MykXqpdHxImUwT9PppzkNnaH6cu1wNSI+CfKifrfUO6o1U4nU17ThZQukhMod8D5j8xc2O+aL/bPUe5QdilwF2Ww1n0piddPNxbMzAcj4keUC48vN7bmi4i/p7Tc+BHlAn9rygXexdW690TENModuV5BaaX3MmC/zDyIMsDqwmr5PwMbUO6W1NhFdDjU2dd/GxHPUpLDf0u5mDkYSmu7KLf4Pi8iFlHqzTso9fMfe+lS2qyvOr6czLwmIn4JXBblzmeLKO/jSyhdq1oWEVdRLtRvp3wu9qOcu/1Pf+u12b9QjpNLDcG+HaybKeP+/EeUuziOpQxOXavOZubjETGL0iLsCcpx5wTKsa7XREs/23q+iuX0iHiE8h30ARoSOG3Q00rr2Ij4GfBE1R22L7dS9tG51We9Z+D8pS0Yq1Y9H6EMCv57Sveyv2f5Ma6aDeVxsS9jIuKvKAOiv4rSfesISp1rvPtZK9+vfX2mrwWOiYhbKGNLHUo5lgyZoTxO1HBNRNxNucPn/ZT9dzSlhdp5VVzPRsR9wAER8b+UxNGLWntn5qLq++QLEbEEmEs5Z1q/qehQnzdJGoFsqSRpOBwM/LY5oQSQmc9Rxm/YN8qtaActMx+knLh8l3KB8BPKHVJmU+6sNbeX1aZTxgB4gXIy3bi9xZRkyrWUi+0fA+dQLt5buk1zk09QLjimUn4B/l8GviUwlBPoH1O6Np1P+SVwen8rDIGbgLcDF1KSQO8H/i4zvweQmb+jnBDvS7kt939X8/9I2WeLKS3TplJe8x4943xkZlISjXdX2z+H8ovznTRc5PTjPMqgo8dS3uutKHfqaZvM/DHlV+8uymv9ZBXD0YPY3CWUC5fjKeMXXUxpDXBwZp7RS/meenlh0/w7KF0Gz6TUj89RbqP92YYyH6fU3cMo7+PZVAnJzHyG8v4tAa6gXAx+mTKg6rCpua8PotTN71EGPj8wyyD8Pdv6D0o9eT9lQNyDgeOrlncDxdFrHe/D+yjHibMpXXwDeGdmzutnnd78strWpZQBcN8MfGCALoTtdgXLkhhLrci+HawsN1HYn3JxfDWlbhzJsgG96ziEcsF7MeUYdGX1/2CcTUm+HVltZ31K0qZdfk5JRBxLSayf11/h6tj7fsr33BWUY8/HKHfC7DGP0mXrXyjHk9MoCeuP9LPdoTwu9uUQyvfRdZS7km5JucPY31TnDz0G/H7t5zN9CuX76kvV32eBY4bwNfQYquNEq06j/KD3Vcr50FmUxOnbMrOxJdmRlPOgn1BaEm/ex/Y+Q9m3J1H204OU75+l2nDeJGkEinIuL0nSyFGNiTIf+NfMPHm44xlJIuJyYExm7jrcsYwUEXEEJcm2QZa7IUqSJGklsPubJGnYRcSRlF/Nf0tpbXMcpavCBcMZ10gSERMoLQD2ZeCxtyRJkqS2M6kkSRoJFlO6aW1F6XJxK/CuqnuCiv+mdEn4t8y8YriDkSRJkuz+JkmSJEmSpNocqFuSJEmSJEm1mVSSJEmSJElSbSaVJEmSJEmSVJtJJUmSJEmSJNVmUkmSJEmSJEm1mVSSJEmSJElSbSaVJEmSJEmSVJtJJUmSJEmSJNVmUkmSJEmSJEm1jW7nxiNiEnAOMAr4ZmZ+pY9yHwCuAHbKzNnVvBOBjwLPA8dk5jX9Pdemm26a48aNG8LoJUmSJEmS1my33XbbI5m5WW/L2pZUiohRwFRgD2AhMCsiZmTmnU3lNgCOBW5pmLcdcBDwBmBz4CcR8brMfL6v5xs3bhyzZ88e+hciSZIkSZK0hoqI3/W1rJ3d33YG5mXm/Mx8FpgOTO6l3BeBrwKLG+ZNBqZn5jOZeR8wr9qeJEmSJEmSRoB2JpW2AO5vmF5YzVsqIt4EbJmZP6i7riRJkiRJkoZPW8dU6k9ErAWcCRyxAtuYAkwBGDNmDN3d3UMTnCRJkiRJkvrVzqTSA8CWDdNjq3k9NgDeCFwfEQCvAmZExD4trAtAZk4DpgF0dXVlZ2fnUMYvSZIkSZJWIc899xwLFy5k8eLFAxfWcjo6Ohg7dixrr712y+u0M6k0C9g6IsZTEkIHAYf0LMzMx4FNe6Yj4nrg05k5OyKeBi6NiDMpA3VvDdzaxlglSZIkSdIqbuHChWywwQaMGzeOqgGLWpCZLFq0iIULFzJ+/PiW12vbmEqZuQQ4GrgGuAu4PDPnRsQpVWuk/tadC1wO3An8CDiqvzu/SZIkSZIkLV68mE022cSEUk0RwSabbFK7hVdbx1TKzJnAzKZ5J/VRdrem6VOBU9sWnCRJkiRJWu2YUBqcwey3dt79TZIkSZIkaY0SERx//PFLp08//XQ+//nPr9QYdtttN2bPnt325xm2u79JkiRJkiS104nfnTOk2/vyvhMGLLPuuuvy3e9+lxNPPJFNN910wPLNlixZwujRq0a6ZtWIUpIkSZIkaRUwevRopkyZwllnncWppy4/qs+CBQv4yEc+wiOPPMJmm23GhRdeyKtf/WqOOOIIOjo6uP3225k4cSKPPvooL3nJS7j99tt5+OGHueCCC7j44ou56aab2GWXXbjooosA+NjHPsasWbN4+umn2W+//fjCF76wUl+r3d8kSZIkSZKG0FFHHcUll1zC448/vtz8T3ziExx++OHccccdHHrooRxzzDFLly1cuJBf/vKXnHnmmQA89thj3HTTTZx11lnss88+fOpTn2Lu3LnMmTOH7u5uAE499VRmz57NHXfcwQ033MAdd9yx8l4kJpUkSZIkSZKG1IYbbsiHPvQhzj333OXm33TTTRxyyCEAfPCDH+TGG29cumz//fdn1KhRS6ff+973EhFMmDCBV77ylUyYMIG11lqLN7zhDSxYsACAyy+/nDe96U3suOOOzJ07lzvvvLP9L66BSSVJkiRJkqQh9slPfpLzzz+fp556qqXy66233nLT6667LgBrrbXW0v97ppcsWcJ9993H6aefzk9/+lPuuOMO9t57bxYvXjx0L6AFjqk0Ag31QGLqWyuDrEmSJEmSVNfGG2/MAQccwPnnn89HPvIRAN761rcyffp0PvjBD3LJJZew6667Dnr7TzzxBOuttx4bbbQRDz30ED/84Q/Zbbfdhij61thSSZIkSZIkqQ2OP/54HnnkkaXTX//617nwwgvZfvvt+fa3v80555wz6G3vsMMO7LjjjmyzzTYccsghTJw4cShCriUyc6U/aTt0dXXl7NmzhzuMIWFLpZXHlkqSJEmStPq466672HbbbYc7jFVWb/svIm7LzK7eyttSSZIkSZIkSbWZVJIkSZIkSVJtJpUkSZIkSZJUm0klSZIkSZIk1WZSSZIkSZIkSbWZVJIkSZIkSVJtJpUkSZIkSZKGyPrrrz9gmbPPPpu//OUvQ/J8Dz74IPvtt9+QbKuu0cPyrJIkSZIkSe128TlDu70PHTskmzn77LM57LDDeOlLX/qiZc8//zyjRo1qeVubb745V1xxxZDEVZctlSRJkiRJkobY9ddfz2677cZ+++3HNttsw6GHHkpmcu655/Lggw+y++67s/vuuwOlddPxxx/PDjvswE033cS4ceN45JFHAJg9eza77bYbADfccAOdnZ10dnay44478uSTT7JgwQLe+MY3ArB48WI+/OEPM2HCBHbccUeuu+46AC666CL23XdfJk2axNZbb81nPvOZIXmNtlSSJEmSJElqg9tvv525c+ey+eabM3HiRH7xi19wzDHHcOaZZ3Ldddex6aabAvDUU0+xyy67cMYZZ/S7vdNPP52pU6cyceJE/vznP9PR0bHc8qlTpxIRzJkzh7vvvps999yT3/zmNwB0d3dz++23s+666/L617+eT3ziE2y55ZYr9PpsqSRJkiRJktQGO++8M2PHjmWttdais7OTBQsW9Fpu1KhRfOADHxhwexMnTuS4447j3HPP5U9/+hOjRy/fVujGG2/ksMMOA2CbbbZhq622WppU+uu//ms22mgjOjo62G677fjd7363Yi8Ok0qSJEmSJEltse666y79f9SoUSxZsqTXch0dHcuNozR69GheeOEFoHRp63HCCSfwzW9+k6effpqJEydy9913D3ksdZhUkiRJkiRJWok22GADnnzyyT6Xjxs3jttuuw2AK6+8cun8e++9lwkTJvDZz36WnXba6UVJpV133ZVLLrkEgN/85jf8/ve/5/Wvf30bXkFhUkmSJEmSJGklmjJlCpMmTVo6UHezk08+mWOPPZaurq7lWjCdffbZvPGNb2T77bdn7bXXZq+99lpuvY9//OO88MILTJgwgQMPPJCLLrpouRZKQy0ys20bX5m6urpy9uzZwx3GkDjxu3OGO4Q1xpf3nR6bYKQAACAASURBVDDcIUiSJEmShshdd93FtttuO9xhrLJ6238RcVtmdvVW3pZKkiRJkiRJqs2kkiRJkiRJkmozqSRJkiRJkqTaTCpJkiRJkqTVxuoydvTKNpj9ZlJJkiRJkiStFjo6Oli0aJGJpZoyk0WLFtHR0VFrvdFtikeSJEmSJGmlGjt2LAsXLuSPf/zjcIeyyuno6GDs2LG11jGpJEmSJEmSVgtrr70248ePH+4w1hht7f4WEZMi4p6ImBcRJ/Sy/MiImBMR3RFxY0RsV80fFxFPV/O7I+Ib7YxTkiRJkiRJ9bStpVJEjAKmAnsAC4FZETEjM+9sKHZpZn6jKr8PcCYwqVp2b2Z2tis+SZIkSZIkDV47WyrtDMzLzPmZ+SwwHZjcWCAzn2iYXA9wJC1JkiRJkqRVQDuTSlsA9zdML6zmLScijoqIe4HTgGMaFo2PiNsj4oaI2LWNcUqSJEmSJKmmYR+oOzOnAlMj4hDgc8DhwB+AV2fmooh4M/C9iHhDU8smImIKMAVgzJgxdHd3r+To22N8PDrcIawxVpc6I0mSJEnSytbOpNIDwJYN02OreX2ZDvw7QGY+AzxT/X9b1ZLpdcDsxhUycxowDaCrqys7O1ePIZgumz9nuENYY0zpnDDcIUiSJEmStEpqZ/e3WcDWETE+ItYBDgJmNBaIiK0bJvcGflvN36wa6JuIeA2wNTC/jbFKkiRJkiSphra1VMrMJRFxNHANMAq4IDPnRsQpwOzMnAEcHRHvAp4DHqN0fQN4O3BKRDwHvAAcmZn2CZMkSZIkSRoh2jqmUmbOBGY2zTup4f9j+1jvSuDKdsYmSZIkSZKkwWtn9zdJkiRJkiStpkwqSZIkSZIkqTaTSpIkSZIkSarNpJIkSZIkSZJqM6kkSZIkSZKk2kwqSZIkSZIkqTaTSpIkSZIkSarNpJIkSZIkSZJqM6kkSZIkSZKk2kwqSZIkSZIkqTaTSpIkSZIkSarNpJIkSZIkSZJqM6kkSZIkSZKk2kwqSZIkSZIkqTaTSpIkSZIkSarNpJIkSZIkSZJqM6kkSZIkSZKk2kwqSZIkSZIkqTaTSpIkSZIkSarNpJIkSZIkSZJqM6kkSZIkSZKk2kwqSZIkSZIkqTaTSpIkSZIkSarNpJIkSZIkSZJqM6kkSZIkSZKk2kwqSZIkSZIkqTaTSpIkSZIkSarNpJIkSZIkSZJqM6kkSZIkSZKk2kwqSZIkSZIkqTaTSpIkSZIkSarNpJIkSZIkSZJqG93OjUfEJOAcYBTwzcz8StPyI4GjgOeBPwNTMvPOatmJwEerZcdk5jXtjHUked+cK4Y7hDXHn3/WetkPHdu+OCRJkiRJWsW0raVSRIwCpgJ7AdsBB0fEdk3FLs3MCZnZCZwGnFmtux1wEPAGYBLwb9X2JEmSJEmSNAK0s/vbzsC8zJyfmc8C04HJjQUy84mGyfWArP6fDEzPzGcy8z5gXrU9SZIkSZIkjQDt7P62BXB/w/RCYJfmQhFxFHAcsA7wzoZ1b25ad4v2hClJkiRJkqS62jqmUisycyowNSIOAT4HHN7quhExBZgCMGbMGLq7u9sT5Er2+NgthzuENUZ3R0eNwqtH/ZIkSZIkaSi0M6n0ANCYHRlbzevLdODf66ybmdOAaQBdXV3Z2dm5IvGOGLdcfdVwh7DG6By/cY3Cq0f9kiRJkiRpKLRzTKVZwNYRMT4i1qEMvD2jsUBEbN0wuTfw2+r/GcBBEbFuRIwHtgZubWOskiRJkiRJqqFtLZUyc0lEHA1cA4wCLsjMuRFxCjA7M2cAR0fEu4DngMeour5V5S4H7gSWAEdl5vPtilWSJEmSJEn1tHVMpcycCcxsmndSw//H9rPuqcCp7YtOkiRJkiRJg9XO7m+SJEmSJElaTZlUkiRJkiRJUm0mlSRJkiRJklSbSSVJkiRJkiTVZlJJkiRJkiRJtZlUkiRJkiRJUm0mlSRJkiRJklSbSSVJkiRJkiTVZlJJkiRJkiRJtZlUkiRJkiRJUm0mlSRJkiRJklSbSSVJkiRJkiTVZlJJkiRJkiRJtZlUkiRJkiRJUm0mlSRJkiRJklSbSSVJkiRJkiTVZlJJkiRJkiRJtZlUkiRJkiRJUm0mlSRJkiRJklSbSSVJkiRJkiTVZlJJkiRJkiRJtZlUkiRJkiRJUm0mlSRJkiRJklSbSSVJkiRJkiTVZlJJkiRJkiRJtZlUkiRJkiRJUm0mlSRJkiRJklSbSSVJkiRJkiTVZlJJkiRJkiRJtZlUkiRJkiRJUm0mlSRJkiRJklRbW5NKETEpIu6JiHkRcUIvy4+LiDsj4o6I+GlEbNWw7PmI6K4eM9oZpyRJkiRJkuoZ3a4NR8QoYCqwB7AQmBURMzLzzoZitwNdmfmXiPgYcBpwYLXs6czsbFd8kiRJkiRJGrx2tlTaGZiXmfMz81lgOjC5sUBmXpeZf6kmbwbGtjEeSZIkSZIkDZF2JpW2AO5vmF5YzevLR4EfNkx3RMTsiLg5It7XjgAlSZIkSZI0OG3r/lZHRBwGdAHvaJi9VWY+EBGvAX4WEXMy896m9aYAUwDGjBlDd3f3Sou5nR4fu+Vwh7DG6O7oqFF49ahfkiRJkiQNhXYmlR4AGrMjY6t5y4mIdwH/BLwjM5/pmZ+ZD1R/50fE9cCOwHJJpcycBkwD6Orqys7O1WMIpluuvmq4Q1hjdI7fuEbh1aN+SZIkSZI0FNrZ/W0WsHVEjI+IdYCDgOXu4hYROwLnAftk5sMN818eEetW/28KTAQaB/iWJEmSJEnSMGpbS6XMXBIRRwPXAKOACzJzbkScAszOzBnA14D1ge9EBMDvM3MfYFvgvIh4gZL4+krTXeMkSZIkSZI0jNo6plJmzgRmNs07qeH/d/Wx3i+BCe2MTZIkSZIkSYPXzu5vkiRJkiRJWk2ZVJIkSZIkSVJtJpUkSZIkSZJUm0klSZIkSZIk1WZSSZIkSZIkSbWZVJIkSZIkSVJtJpUkSZIkSZJUW0tJpYg4LSI2jIi1I+KnEfHHiDis3cFJkiRJkiRpZGq1pdKemfkE8B5gAfBa4B/aFZQkSZIkSZJGtlaTSqOrv3sD38nMx9sUjyRJkiRJklYBowcuAsD3I+Ju4GngYxGxGbC4fWFJkiRJkiRpJGuppVJmngC8FejKzOeAp4DJ7QxMkiRJkiRJI1erLZUAtgHGRUTjOhcPcTySJEmSJElaBbSUVIqIbwP/D+gGnq9mJyaVJEmSJEmS1kittlTqArbLzGxnMJIkSZIkSVo1tHr3t/8FXtXOQCRJkiRJkrTqaLWl0qbAnRFxK/BMz8zM3KctUUmSJEmSJGlEazWp9Pl2BiFJkiRJkqRVS0tJpcy8ISJeCexUzbo1Mx9uX1iSJEmSJEkayVoaUykiDgBuBfYHDgBuiYj92hmYJEmSJEmSRq5Wu7/9E7BTT+ukiNgM+AlwRbsCkyRJkiRJ0sjV6t3f1mrq7raoxrqSJEmSJElazbTaUulHEXEN8F/V9IHAzPaEJEmSJEmSpJGu1YG6/yEiPgBMrGZNy8yr2heWJEmSJEmSRrJWWyqRmVcCV7YxFkmSJEmSJK0i+k0qRcSNmfm2iHgSyMZFQGbmhm2NTpIkSZIkSSNSv0mlzHxb9XeDlROOJEmSJEmSVgUt3cEtIr7dyjxJkiRJkiStGVpKKgFvaJyIiNHAm4c+HEmSJEmSJK0K+k0qRcSJ1XhK20fEE9XjSeAh4OqVEqEkSZIkSZJGnH6TSpn5ZWAj4OLM3LB6bJCZm2TmiSsnREmSJEmSJI00A3Z/y8wXgJ0Gs/GImBQR90TEvIg4oZflx0XEnRFxR0T8NCK2alh2eET8tnocPpjnlyRJkiRJUnu0OqbSryKiVmIpIkYBU4G9gO2AgyNiu6ZitwNdmbk9cAVwWrXuxsDJwC7AzsDJEfHyOs8vSZIkSZKk9mk1qbQLcFNE3Fu1KpoTEXcMsM7OwLzMnJ+ZzwLTgcmNBTLzusz8SzV5MzC2+v/dwLWZ+WhmPgZcC0xqMVZJkiRJkiS12egWy717ENveAri/YXohJTnVl48CP+xn3S0GEYMkSZIkSZLaoKWkUmb+LiJ2AHatZv08M389VEFExGFAF/COmutNAaYAjBkzhu7u7qEKaVg9PnbL4Q5hjdHd0VGj8OpRvyRJkiRJGgotJZUi4ljg74DvVrP+MyKmZebX+1ntAaAxOzK2mte87XcB/wS8IzOfaVh3t6Z1r29eNzOnAdMAurq6srOzs5WXM+LdcvVVwx3CGqNz/MY1Cq8e9UuSJEmSpKHQave3jwK7ZOZTABHxVeAmoL+k0ixg64gYT0kSHQQc0lggInYEzgMmZebDDYuuAf6lYXDuPYETW4xVkiRJkiRJbdZqUimA5xumn6/m9Skzl0TE0ZQE0SjggsycGxGnALMzcwbwNWB94DsRAfD7zNwnMx+NiC9SElMAp2Tmoy2/KkmSJEmSJLVVq0mlC4FbIuIqSjJpMnD+QCtl5kxgZtO8kxr+f1c/614AXNBifJIkSZIkSVqJWh2o+8yIuB54G5DAhzPz9nYGJkmSJEmSpJFrrZrlo+mvJEmSJEmS1kAtJZUi4iTgW8DLgU2BCyPic+0MTJIkSZIkSSNXq2MqHQrskJmLASLiK0A38KV2BSZJkiRJkqSRq9Xubw8CHQ3T6wIPDH04kiRJkiRJWhW02lLpcWBuRFxLGah7D+DWiDgXIDOPaVN8kiRJkiRJGoFaTSpdVT16XD/0oUiSJEmSJGlV0VJSKTO/FRHrAK+rZt2Tmc+1LyxJkiRJkiSNZC0llSJiN8rd3xYAAWwZEYdn5v+0LzRJkiRJkiSNVK12fzsD2DMz7wGIiNcB/wW8uV2BSZIkSZIkaeRq9e5va/cklAAy8zfA2u0JSZIkSZIkSSNdqy2VbouIbwL/WU0fCsxuT0iSJEmSJEka6VpNKh0JHAUcU03/HPi3tkQkSZIkSZKkEW/ApFJEjAJ+nZnbAGe2PyRJkiRJkiSNdAOOqZSZzwP3RMSrV0I8kiRJkiRJWgW02v3t5cDciLgVeKpnZmbu05aoJEmSJEmSNKK1mlT657ZGIUmSJEmSpFVKv0mliOigDNL9WmAOcH5mLlkZgUmSJEmSJGnkGmhMpW8BXZSE0l7AGW2PSJIkSZIkSSPeQN3ftsvMCQARcT5wa/tDkiRJkiRJ0kg3UEul53r+sdubJEmSJEmSegzUUmmHiHii+j+Al1TTAWRmbtjW6CRJkiRJkjQi9ZtUysxRKysQSZIkSZIkrToG6v4mSZIkSZIkvYhJJUmSJEmSJNVmUkmSJEmSJEm1mVSSJEmSJElSbSaVJEmSJEmSVJtJJUmSJEmSJNVmUkmSJEmSJEm1mVSSJEmSJElSbW1NKkXEpIi4JyLmRcQJvSx/e0T8KiKWRMR+Tcuej4ju6jGjnXFKkiRJkiSpntHt2nBEjAKmAnsAC4FZETEjM+9sKPZ74Ajg071s4unM7GxXfJIkSZIkSRq8tiWVgJ2BeZk5HyAipgOTgaVJpcxcUC17oY1xSJIkSZIkaYi1s/vbFsD9DdMLq3mt6oiI2RFxc0S8b2hDkyRJkiRJ0opoZ0ulFbVVZj4QEa8BfhYRczLz3sYCETEFmAIwZswYuru7hyPOIff42C2HO4Q1RndHR43Cq0f9kiRJkiRpKLQzqfQA0JgdGVvNa0lmPlD9nR8R1wM7Avc2lZkGTAPo6urKzs7VYwimW66+arhDWGN0jt+4RuHVo35JkiRJkjQU2tn9bRawdUSMj4h1gIOAlu7iFhEvj4h1q/83BSbSMBaTJEmSJEmShlfbkkqZuQQ4GrgGuAu4PDPnRsQpEbEPQETsFBELgf2B8yJibrX6tsDsiPg1cB3wlaa7xkmSJEmSJGkYtXVMpcycCcxsmndSw/+zKN3imtf7JTChnbFJkiRJkiRp8NrZ/U2SJEmSJEmrKZNKkiRJkiRJqs2kkiRJkiRJkmozqSRJkiRJkqTaTCpJkiRJkiSpNpNKkiRJkiRJqs2kkiRJkiRJkmozqSRJkiRJkqTaTCpJkiRJkiSpNpNKkiRJkiRJqs2kkiRJkiRJkmozqSRJkiRJkqTaTCpJkiRJkiSpNpNKkiRJkiRJqs2kkiRJkiRJkmozqSRJkiRJkqTaTCpJkiRJkiSpNpNKkiRJkiRJqs2kkiRJkiRJkmozqSRJkiRJkqTaTCpJkiRJkiSpNpNKkiRJkiRJqs2kkiRJkiRJkmozqSRJkiRJkqTaTCpJkiRJkiSpNpNKkiRJkiRJqs2kkiRJkiRJkmozqSRJkiRJkqTaTCpJkiRJkiSpNpNKkiRJkiRJqs2kkiRJkiRJkmpra1IpIiZFxD0RMS8iTuhl+dsj4lcRsSQi9mtadnhE/LZ6HN7OOCVJkiRJklRP25JKETEKmArsBWwHHBwR2zUV+z1wBHBp07obAycDuwA7AydHxMvbFaskSZIkSZLqaWdLpZ2BeZk5PzOfBaYDkxsLZOaCzLwDeKFp3XcD12bmo5n5GHAtMKmNsUqSJEmSJKmGdiaVtgDub5heWM1r97qSJEmSJElqs9HDHcCKiIgpwBSAMWPG0N3dPcwRDY3Hx2453CGsMbo7OmoUXj3qlyRJkiRJQ6GdSaUHgMbsyNhqXqvr7ta07vXNhTJzGjANoKurKzs7OwcT54hzy9VXDXcIa4zO8RvXKLx61C9JkiRJkoZCO7u/zQK2jojxEbEOcBAwo8V1rwH2jIiXVwN071nNkyRJkiRJ0gjQtqRSZi4BjqYkg+4CLs/MuRFxSkTsAxARO0XEQmB/4LyImFut+yjwRUpiahZwSjVPkiRJkiRJI0Bbx1TKzJnAzKZ5JzX8P4vSta23dS8ALmhnfJIkSZIkSRqcdnZ/kyRJkiRJ0mrKpJIkSZIkSZJqM6kkSZIkSZKk2kwqSZIkSZIkqTaTSpIkSZIkSarNpJIkSZIkSZJqM6kkSZIkSZKk2kwqSZIkSZIkqTaTSpIkSZIkSarNpJIkSZIkSZJqM6kkSZIkSZKk2kwqSZIkSZIkqTaTSpIkSZIkSarNpJIkSZIkSZJqM6kkSZIkSZKk2kwqSZIkSZIkqTaTSpIkSZIkSarNpJIkSZIkSZJqM6kkSZIkSZKk2kwqSZIkSZIkqTaTSpIkSZIkSarNpJIkSZIkSZJqGz3cAUjSoF18znBHoN586NjhjkCSJEnSSmBLJUmSJEmSJNVmUkmSJEmSJEm1mVSSJEmSJElSbSaVJEmSJEmSVJtJJUmSJEmSJNVmUkmSJEmSJEm1jR7uACStGU787pwh3+b77nt0yLe5Othl/MbDHYIkSZKkNYAtlSRJkiRJklSbSSVJkiRJkiTV1takUkRMioh7ImJeRJzQy/J1I+KyavktETGumj8uIp6OiO7q8Y12xilJkiRJkqR62jamUkSMAqYCewALgVkRMSMz72wo9lHgscx8bUQcBHwVOLBadm9mdrYrPkmSJEmSJA1eO1sq7QzMy8z5mfksMB2Y3FRmMvCt6v8rgL+OiGhjTJIkSZIkSRoC7UwqbQHc3zC9sJrXa5nMXAI8DmxSLRsfEbdHxA0RsWsb45QkSZIkSVJNbev+toL+ALw6MxdFxJuB70XEGzLzicZCETEFmAIwZswYuru7hyHUoff42C2HO4Q1RndHR43Cq0f9Gi7j49Eh36afld7VqtdtCcDPiiRJkrQmaGdS6QGg8YpvbDWvtzILI2I0sBGwKDMTeAYgM2+LiHuB1wGzG1fOzGnANICurq7s7Fw9hmC65eqrhjuENUbn+I1rFF496tdwuWz+nCHf5g4Lbxjyba4OatXrtgTgZ0WSJElaE7QzqTQL2DoixlOSRwcBhzSVmQEcDtwE7Af8LDMzIjYDHs3M5yPiNcDWwPw2xqo11C33td565nvfHfqkiCRJkiRJq6q2JZUyc0lEHA1cA4wCLsjMuRFxCjA7M2cA5wPfjoh5wKOUxBPA24FTIuI54AXgyMwc+r4zkiRJkiRJGpS2jqmUmTOBmU3zTmr4fzGwfy/rXQlc2c7YJEmSJEmSNHgjdaBuacR535wrhjsESZIkSZJGjLWGOwBJkiRJkiStekwqSZIkSZIkqTaTSpIkSZIkSarNpJIkSZIkSZJqM6kk/f/27j3IkrK84/j3x65gEAIKSBRQUFGDIAgGNSoiKZQoBYiiGC94KSkqmICxjK4a46WMGssYSzSJxWVRUWMQFFC5KKBWBbm6sNw0CCgLGEQ0isrCwpM/utcdhjnLnN2Z6eme76dqa053nzPzPNtvX87Tb78tSZIkSZLGZlFJkiRJkiRJY1vcdQCSJElrs+SU5V2HsGB86OBdug5BkiT1iD2VJEmSJEmSNDaLSpIkSZIkSRqbRSVJkiRJkiSNzaKSJEmSJEmSxmZRSZIkSZIkSWPz6W+SpJn12U90HYEG5qAb7ug6hIXjznO7jqBbrz2q6wgkSeoVeypJkiRJkiRpbBaVJEmSJEmSNDaLSpIkSZIkSRqbYypJkrQOLnScH0mSNF84puX8tADG6rOnkiRJkiRJksZmUUmSJEmSJEljs6gkSZIkSZKksTmmkiQNjGP9SJIkSZoL9lSSJEmSJEnS2CwqSZIkSZIkaWwWlSRJkiRJkjQ2i0qSJEmSJEkam0UlSZIkSZIkjc2ikiRJkiRJksa2uOsAJEmSND9ceMMdXYfQqa+esnzO/taHDt5lzv6WJEmzxZ5KkiRJkiRJGps9lSRJkiQN0pI57H220Nn7TlqYZrWolGQ/4BPAIuDYqvrwpOUbAZ8F9gB+Abyiqm5sly0B3gjcC/xtVZ01m7FKkiRJc8Vih/rsoOUnP3DmnefOfSCSOjdrRaUki4BPAfsCK4CLk5xWVVdPeNsbgV9W1ROSHAp8BHhFkp2AQ4GnAI8GvpXkiVV172zFK0mSJGl+mbJ4oXlpoY/JNpeescMjug5B+oPZ7Km0J3BdVV0PkORLwIHAxKLSgcB729cnA8ckSTv/S1W1ErghyXXt77tgFuOVJEnSAmYBQ5Kk8czmQN3bADdNmF7RzpvyPVW1Cvg/YItpflaSJEmSJEkd6fVA3UkOBw5vJ+9M8sMu45lBWwK3dx3ELBhiXubUH0PMa4g5wTDzMqf+GGJeQ8wJhpmXOfXHEPMaYk4wzLzMqS8OO3ooeT121ILZLCrdDGw3YXrbdt5U71mRZDGwGc2A3dP5LFX1GeAzMxjzvJDkkqp6etdxzLQh5mVO/THEvIaYEwwzL3PqjyHmNcScYJh5mVN/DDGvIeYEw8zLnPpjqHlNNJu3v10M7JhkhyQb0gy8fdqk95wGHNa+fhlwblVVO//QJBsl2QHYEbhoFmOVJEmSJEnSGGatp1JVrUryZuAsYBFwfFVdleT9wCVVdRpwHPC5diDuO2gKT7Tv+zLNoN6rgCN98pskSZIkSdL8MatjKlXVN4BvTJr3ngmv7wIOGfHZDwIfnM345rHB3dLXGmJe5tQfQ8xriDnBMPMyp/4YYl5DzAmGmZc59ccQ8xpiTjDMvMypP4aa1x+kudtMkiRJkiRJmr7ZHFNJkiRJkiRJA2VRaR5IsijJD5Kc0U4vTXJDkmXtv926jnF9Tc6xb5Jsl+S8JFcnuSrJUZOWvzVJJdmyqxjXV5KHJrkoyeVtju/rOqZ1leSoJFe2eRzdzntEknOS/E/78+Fdx7k+kmye5OQk1ya5Jsmzuo5pXYxYV4e00/cl6cXTMpIcn+S2JFdOmDdlm0uyWZLTJ2xrr+8u8umb4lj1vQnHqVuSfLXrGNdHkhuTLG/zuaTreMYxov19tN0/XJHk1CSbt/O3T/L7Cevu37uLfHpG5LdrkgvadXZ6kj/uMsZ1MVWbG8Kxam3bUl/Ol0a0uQ+029OyJGcnefSEZXu3869K8p1uoh7fFPv1fZJc1h6XT0zzdO55bZzjb7usl+tqtanyHYIk+yX5YZLrkryj63ima0T7m/I8NsmGSU5o94+XJ9m7k6BngUWl+eEo4JpJ895WVbu1/5Z1EdQMmyrHPlkFvLWqdgKeCRyZZCdoCk7AC4CfdhjfTFgJ7FNVuwK7AfsleWbHMY0tyc7Am4A9gV2B/ZM8AXgH8O2q2hH4djvdZ58AzqyqJ9Pk2bvtay3r6krgYOC7HYY3rqXAfpPmjWpzRwJXt9va3sDH0jwldb673368qp67+jgFXACc0llkM+f5bU69KGZOsJQHtr9zgJ2r6qnAj4AlE5b9eMI5xhFzFOP6WMoD8zsWeEdV7QKcCrxtroOaIZPb3FCOVQ/Ylnp2vrSUB7a5j1bVU9t93hnAe6C5yAN8Gjigqp7CiPFi56k/7NeTbACcCBxaVTsDP2HNU7rns6VM8/jb83W12lIemG+vJVkEfAr4S2An4JWrv2f1wFIeuD5Gnce+CaA9bu1Lc/43iHrMIJLosyTbAi+mOTkapCHkWFW3VtVl7evf0ByAt2kXfxz4e6DXA5RV48528iHtvz7m9KfAhVX1u6paBXyHZsd+IM3JEu3PgzqKb70l2QzYi+YJmlTV3VX1q26jWidTrququqaqfthxbGOpqu/SPMV0olFtroBNkwTYpP3cqrmIc12tbT/e9hDZB+h1T6U+m6r9VdXZ7XYF8H1g2zkPbIaM2L6eyJoT9nOAl85pULNnMMeqKfTmfGnENvXrCZMPY00efwWcUlU/bd9325wEuZ6m2K9vAdxdVT9qp3uxXY15/O3luppoRL59tydwXVVdX1V3A1+iWYfz3oh9xajz2J2Ac9v33Ab8CujbRawpWVTq3r/SHGDvmzT/g20X248n2aiDuGbSqBx7Kcn2wNOAC5McCNxcVZd3GtQMabtBLwNuA86pqgu7jmkdXAk8N8kWSTYGXgRsB2xdVbe27/kZsHVXAc6AHYCfAye03daPTfKwroNaB6PW1VCManPH0BTUbgGWA0dV1Xzf3wT7qAAACEtJREFUP65tP34QzRXhX0+xrE8KODvJpUkO7zqYGfYG4JsTpndo9x3fSfLcroJaT1ex5kvHIfRz3zFVmxvCseoBeQ3lfCnJB5PcBLyKtqcSTYHz4UnOb3N+bXcRjmXyfv12YPGE23VeRj+3Kxi9HfV1XQ3dNsBNE6ZXsObi/ZBcDhyQZHGSHYA96O82dj8WlTqUZH/gtqq6dNKiJcCTgT8DHgG8fa5jmylrybGXkmwCfAU4mqZnwTtZc1LRe1V1b9ute1tgz/b2pF6pqmuAjwBnA2cCy4B7J72n6MGV0rVYDOwO/FtVPQ34LT28RWI662ooJrW5F9Lk+miaW02PyTweD2Ya+/FXAl+cw5Bmy3Oqanea7vdHJtmr64BmQpJ30RyvTmpn3Qo8pt13/B3whfnc/tbiDcBfJ7kU2BS4u+N41sVa21yPj1VT5TWI86WqeldVbUezPb25nb2Y5svhi2n27/+Q5IkdhTgtU+3X2/Z2KPDxJBcBv2EAx+RJ21Hv1pUG5XiagtklNEXd/2YA2xhYVOras2mqlTfSdPPbJ8nn21utqqpWAifQdAnsqylz7DakdZPkITQFpZOq6hTg8TQ9Ri5v89sWuCzJn3QX5cxob6U6j57es11Vx1XVHlW1F/BLmvFE/jfJowDan73r8jzBCmDFhJ5kJ9MUmXpnxLoailFt7vU03e+rqq4DbqC5kDBfjdyPpxlsd0/g692FNzOq6ub25200Y/T0+dgLQJLXAfsDr2q/WFFVK6vqF+3rS4Ef01y975WquraqXlBVe9AUNX/cdUzjGtHmen+smiKv5zG886WTWHNr2ArgrKr6bVXdTnNb5q6dRTY9o76DXFDNeHl70uTR12PyqO2oj+tqIbiZ+/fY2badNyhVtaqq3tKON3cgsDn93cbux6JSh6pqSVVtW1Xb01wZOLeqXj1hJxia2wp6O7r/qBw7Dmts7bo4Drimqv4FoKqWV9Ujq2r7Nr8VwO5V9bMOQ11nSbbKmqcD/RHNAHLXdhvVuknyyPbnY2jGU/oCcBprBpw8DPhaN9Gtv7aN3ZTkSe2svwCu7jCkdTZiXQ3FqDb3U5p1RpKtgScB1895dNP0IPvxlwFnVNVdnQU4A5I8LMmmq1/TDCbc22MvNE/Sobm15YCq+t2E+Vu1g6KS5HHAjszj9jfKhH3HBsC7gXn/FLuJ1tLmen2sGpHXxUM4X0qy44TJA1lzjvQ14DntLS0bA89gnj88Yy3fQVZvVxvR3CnRq+1qglHbUe/W1QJxMbBjkh3SPLjkUJp1OChJNl49XEWSfYFVVdXL8/fJ5v1jIheok5JsBYTmFok+PJll6J4NvAZY3o45BPDOqvpGhzHNtEcBJ7ZfNjYAvlxVZ3Qc07r6SpItgHuAI6vqV0k+DHw5yRtpnmjy8k4jXH9/Q7Ov2JDmC2EvHks/hanW1UuATwJbAV9PsqyqXthplA8iyRdpnuS2ZZIVwD8Co9rcB4ClSZbT7Off3l4x7aNDafLsu62BU5vrBywGvlBVZ3Yb0vSNaH9LgI2Ac9q8vl/Nk972At6f5B6asVSOqKp5PejriPw2SXJk+5ZTaHp298mUbS7JxfT7WNXrbWm1EW3uRe3FnPto1s0R0NzKneRM4Ip22bFV1dei9NvaW+M2oLnF/tyuA3ow4xx/h7Cupsq3qo7rNqr1U1WrkrwZOAtYBBxfVVd1HNa0jGh/dzD1eewjgbOS3EfTE+s13UQ989L2hpYkSZIkSZKmzdvfJEmSJEmSNDaLSpIkSZIkSRqbRSVJkiRJkiSNzaKSJEmSJEmSxmZRSZIkSZIkSWOzqCRJkha0JO9KclWSK5IsS/KMOfibd7Y/t0/Sq0daS5Ikrba46wAkSZK6kuRZwP7A7lW1MsmWwIYdhyVJktQL9lSSJEkL2aOA26tqJUBV3V5VtyTZJ8lXV78pyb5JTk2yKMnSJFcmWZ7kLe3yJyT5VpLLk1yW5PFJNkny7XZ6eZIDpxtUkr2TnJ/k5CTXJjkpSdplN7bFL5I8Pcn57ev3JjkxyfeS/CTJwUn+uf3bZyZ5yMz9t0mSJFlUkiRJC9vZwHZJfpTk00me184/D3hykq3a6dcDxwO7AdtU1c5VtQtwQrv8JOBTVbUr8OfArcBdwEuqanfg+cDHVheGpulpwNHATsDjgGdP4zOPB/YBDgA+D5zXxvl74MVj/G1JkqQHZVFJkiQtWFV1J7AHcDjwc+A/k7yuqgr4HPDqJJsDzwK+CVwPPC7JJ5PsB/w6yaY0haZT2995V1X9DgjwT0muAL4FbANsPUZ4F1XViqq6D1gGbD+Nz3yzqu4BlgOLgDPb+cun+XlJkqRpc0wlSZK0oFXVvcD5wPlJlgOHAUtpeiGdTtPj6L+qahXwyyS7Ai8EjgBeDhw14le/CtgK2KOq7klyI/DQMUJbOeH1vaw5b1vFmguDk3/f6tv47ktyT1scA7gPz/skSdIMs6eSJElasJI8KcmOE2btBvwEoKpuAW4B3k17m1s7ltEGVfWVdv7uVfUbYEWSg9r3bJRkY2Az4La2oPR84LEzFPaNNL2rAF46Q79TkiRpbF6xkiRJC9kmwCfbW9xWAdfR3Aq32knAVlV1TTu9DXBCktUX5pa0P18D/EeS9wP3AIe0nz297f10CXDtDMX8PuC4JB+g6WElSZLUiazpFS1JkqSJkhwD/KCqjus6FkmSpPnGopIkSdIUklwK/BbYt6pWPtj7JUmSFhqLSpIkSZIkSRqbA3VLkiRJkiRpbBaVJEmSJEmSNDaLSpIkSZIkSRqbRSVJkiRJkiSNzaKSJEmSJEmSxmZRSZIkSZIkSWP7f1O4oS1+WCtLAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1440x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk_fLEaHr0SL"
      },
      "source": [
        "**Labeling Sequences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KILRKFiA2EV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79ef603a-30c0-4ae0-d13b-f1be3a02814a"
      },
      "source": [
        "# Intrusion sequences is labeled 1. Rename columns of the dataframe\n",
        "intrusion['Label'] = 1\n",
        "print(intrusion.head(5), '\\n')"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    0   1   2    3   4   5   6   7    8    9  Label\n",
            "0  45  45  45   45  45  45  45  45   19    6      1\n",
            "1   4   4   4    4   4   4  45  45    5  108      1\n",
            "2  45  45  45   45  45  45  19   6   91  108      1\n",
            "3  90   3  45   45  45  19   6  91  108   90      1\n",
            "4  19   6  91  108  90   3   4   4   45    4      1 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oN2KngnOyG5E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea4e2974-8b1f-4281-d3bd-278b03c03027"
      },
      "source": [
        "# Normal sequences is labeled 0. Rename columns of the dataframe\n",
        "normal['Label'] = 0\n",
        "print(normal.head(5), '\\n')\n",
        "\n",
        "print('Normal len:',len(normal),'\\nIntrusion len:', len(intrusion))"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    0   1   2   3   4    5   6   7   8    9  Label\n",
            "0  45  45  45  45  45   45  45  45  19    6      0\n",
            "1   4   4   4   4   4    4  45  45   5  108      0\n",
            "2  45  45  45  45  45   45  19   6  91  108      0\n",
            "3   4  45   4   3   5  108  90   4   4    4      0\n",
            "4  45  45  45  45  45   45  45  45  45   45      0 \n",
            "\n",
            "Normal len: 373 \n",
            "Intrusion len: 315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ohcyx-nmoYj"
      },
      "source": [
        "## **Partition Training and Testing dataset 70/30**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQ7j7n01sZE1"
      },
      "source": [
        "If there is not enough data from either class, bootstrap to generate more data and create a balanced sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dcj-XXjMZ57h",
        "outputId": "43d24b2d-053c-452c-bd42-303ce7ded27a"
      },
      "source": [
        "# combine normal and intrusion data and split them into training and testing sets\n",
        "df = normal.append(intrusion, ignore_index=True).astype(int)\n",
        "print('Df sz:', df.shape)\n",
        "\n",
        "# Spliting into training and testing\n",
        "x_train, x_test, y_train, y_test = train_test_split(df.iloc[:,:-1], df['Label'], test_size = 0.30, shuffle=True)\n",
        "\n",
        "# Reset index of training and testing sets\n",
        "x_train.reset_index(drop=True, inplace=True); y_train.reset_index(drop=True, inplace=True)\n",
        "x_test.reset_index(drop=True, inplace=True);  y_test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "print('Train sz:',len(x_train), len(y_train))\n",
        "intrusion_train = y_train.loc[y_train == 1]\n",
        "normal_train = y_train.loc[y_train == 0]\n",
        "print('Train set: Intrusion vs. Normal cases', len(y_train.iloc[intrusion_train] ), len(y_train.iloc[normal_train] ))\n",
        "\n",
        "print('Test sz:', len(x_test), len(y_test))\n",
        "intrusion_test = y_test.loc[y_test == 1]\n",
        "normal_test = y_test.loc[y_test == 0]\n",
        "print('Test set: Intrusion vs. Normal cases', len(y_test.iloc[intrusion_test] ), len(y_test.iloc[normal_test] ))"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Df sz: (688, 11)\n",
            "Train sz: 481 481\n",
            "Train set: Intrusion vs. Normal cases 223 258\n",
            "Test sz: 207 207\n",
            "Test set: Intrusion vs. Normal cases 92 115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "c7NZyzbVcNTk",
        "outputId": "973eaf45-1ada-46e1-b5ca-95f7a89f237a"
      },
      "source": [
        "# Bootstrap training data\n",
        "\n",
        "## Lived-name has more intrusion cases than normal cases (189 > 71) --> bootstrap normal cases only\n",
        "x_train['Label'] = y_train\n",
        "\n",
        "if len(intrusion_train) > len(normal_train):\n",
        "  x_train = x_train.iloc[intrusion_train.index].append(x_train.iloc[normal_train.index].sample(n = len(intrusion_train), replace=True), ignore_index=True) #upsampled normal data and add to train set\n",
        "else:\n",
        "  x_train = x_train.iloc[normal_train.index].append(x_train.iloc[intrusion_train.index].sample(n = len(normal_train), replace=True), ignore_index=True) #upsampled intrusion data and add to train set\n",
        "\n",
        "#x_train = x_train.append(x_train.sample(frac=1), ignore_index=True) # Bootstrap training data in case there is not enough data\n",
        "x_train = x_train.sample(frac= SZ) # Shuffle data with a SZ proportion\n",
        "x_train.reset_index(drop=True, inplace=True)\n",
        "y_train = x_train['Label']\n",
        "x_train.drop(columns='Label', inplace=True)\n",
        "x_train"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>3</td>\n",
              "      <td>45</td>\n",
              "      <td>19</td>\n",
              "      <td>6</td>\n",
              "      <td>91</td>\n",
              "      <td>108</td>\n",
              "      <td>90</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>45</td>\n",
              "      <td>136</td>\n",
              "      <td>49</td>\n",
              "      <td>24</td>\n",
              "      <td>47</td>\n",
              "      <td>50</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>90</td>\n",
              "      <td>6</td>\n",
              "      <td>125</td>\n",
              "      <td>91</td>\n",
              "      <td>125</td>\n",
              "      <td>125</td>\n",
              "      <td>125</td>\n",
              "      <td>125</td>\n",
              "      <td>125</td>\n",
              "      <td>125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>3</td>\n",
              "      <td>45</td>\n",
              "      <td>19</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>90</td>\n",
              "      <td>54</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>511</th>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>3</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>512</th>\n",
              "      <td>4</td>\n",
              "      <td>108</td>\n",
              "      <td>90</td>\n",
              "      <td>54</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>513</th>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>514</th>\n",
              "      <td>45</td>\n",
              "      <td>3</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>19</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>108</td>\n",
              "      <td>90</td>\n",
              "      <td>54</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>516 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      0    1    2   3    4    5    6    7    8    9\n",
              "0    45   45    3  45   19    6   91  108   90    3\n",
              "1    45  136   49  24   47   50    5    4    4    4\n",
              "2    90    6  125  91  125  125  125  125  125  125\n",
              "3     3   45   45  45   45   45    3   45   19    6\n",
              "4    90   54    4   4    4    4    4    4    4    4\n",
              "..   ..  ...  ...  ..  ...  ...  ...  ...  ...  ...\n",
              "511  45   45   45   3   45   45   45   45   45   19\n",
              "512   4  108   90  54    4    4    4    4    4    4\n",
              "513  45   45   45  45   45   45   45    1   -1   -1\n",
              "514  45    3   45  45   45   45   45   45   19    6\n",
              "515   4    4    4   4  108   90   54    4    4    4\n",
              "\n",
              "[516 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnDDh0TWsyS7",
        "outputId": "c111f0cf-4b82-4b3b-841f-a95feaefca1b"
      },
      "source": [
        "y_train"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      0\n",
              "1      0\n",
              "2      0\n",
              "3      1\n",
              "4      0\n",
              "      ..\n",
              "511    1\n",
              "512    0\n",
              "513    0\n",
              "514    1\n",
              "515    0\n",
              "Name: Label, Length: 516, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "JFNhH053qfct",
        "outputId": "3695b35b-4494-40c4-882c-cb04aa6f2c56"
      },
      "source": [
        "x_test"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>45</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>90</td>\n",
              "      <td>90</td>\n",
              "      <td>90</td>\n",
              "      <td>6</td>\n",
              "      <td>125</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>90</td>\n",
              "      <td>90</td>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>3</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>125</td>\n",
              "      <td>125</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>91</td>\n",
              "      <td>108</td>\n",
              "      <td>90</td>\n",
              "      <td>3</td>\n",
              "      <td>45</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202</th>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>136</td>\n",
              "      <td>49</td>\n",
              "      <td>24</td>\n",
              "      <td>47</td>\n",
              "      <td>50</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>203</th>\n",
              "      <td>19</td>\n",
              "      <td>6</td>\n",
              "      <td>91</td>\n",
              "      <td>108</td>\n",
              "      <td>90</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204</th>\n",
              "      <td>45</td>\n",
              "      <td>45</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>205</th>\n",
              "      <td>3</td>\n",
              "      <td>45</td>\n",
              "      <td>19</td>\n",
              "      <td>6</td>\n",
              "      <td>91</td>\n",
              "      <td>108</td>\n",
              "      <td>90</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>206</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>108</td>\n",
              "      <td>90</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>207 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0    1    2    3    4    5   6   7   8   9\n",
              "0      4    4    4    4    4   45   4   4   4   4\n",
              "1     90   90   90    6  125    5   3  90  90  90\n",
              "2     45   45    3   45   45   45  45  45  45  45\n",
              "3    125  125   45   45   45   45  45  45  45  45\n",
              "4      6   91  108   90    3   45   4   4   4   4\n",
              "..   ...  ...  ...  ...  ...  ...  ..  ..  ..  ..\n",
              "202   45   45   45  136   49   24  47  50   5   4\n",
              "203   19    6   91  108   90    3   4   4   4   4\n",
              "204   45   45    1   -1   -1   -1  -1  -1  -1  -1\n",
              "205    3   45   19    6   91  108  90   3   4   4\n",
              "206    4    4    4    4    4  108  90   3   4   4\n",
              "\n",
              "[207 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0UCA-ZuXHX_"
      },
      "source": [
        "\n",
        "# **Performance Measures**\n",
        "\n",
        "\n",
        "1.   Function calc_false_positive: Calculates FPR\n",
        "2.   Function print_performance: Formats printing performance metrics and ROC curve for each model\n",
        "3.   Function color_confusion_matrix: prints out a heatmap of confusion matrix in blue color scale\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XA1PZX4XBNLO"
      },
      "source": [
        "# This function calculate False Positive Rate given a confusion matrix\n",
        "def calc_false_positive (cmatrix):\n",
        "  specificity = cmatrix[0,0]/(cmatrix[0,0] + cmatrix[0,1])\n",
        "  return 1-specificity\n",
        "\n",
        "# This function prints performance metrics and ROC curve given the model name, true labels and predicted labels\n",
        "def print_performance( model_name, true_labels, pred_labels):\n",
        "  # rows are actual, columns are predicted\n",
        "  cmatrix = confusion_matrix(true_labels, pred_labels)\n",
        "  fpr = calc_false_positive(cmatrix)\n",
        "\n",
        "  print('Confusion Matrix: \\n',cmatrix)\n",
        "  print('\\nTesting Accuracy: %.2f'% metrics.accuracy_score(true_labels, pred_labels))\n",
        "  print('Precision:%.2f'%  metrics.precision_score(true_labels, pred_labels))\n",
        "  print('Recall: %.2f'% metrics.recall_score(true_labels, pred_labels))\n",
        "  print('False Positive Rate: %.2f'% fpr)\n",
        "  print('\\nClassification report:', classification_report(true_labels, pred_labels), sep='\\n')\n",
        "  print('AUC: %.2f'% roc_auc_score(true_labels, pred_labels))\n",
        "\n",
        "  false_positive_rate, recall, thresholds = roc_curve(true_labels, pred_labels)\n",
        "  roc_auc = auc(false_positive_rate, recall)\n",
        "  plt.figure()\n",
        "  if CLEAN: clean_status='Clean '\n",
        "  else: clean_status ='Overlapped and Duplicated '\n",
        "  plt.title( model_name+' ROC Curve on '+ clean_status + DATA + ' with Seq Len of '+ str(SEQ_WINDOW))\n",
        "  plt.plot(false_positive_rate, recall, 'b', label = 'AUC = %0.2f' %roc_auc)\n",
        "  plt.legend(loc='lower right')\n",
        "  plt.plot([0,1], [0,1], 'r--')\n",
        "  plt.xlim([0.0,1.0])\n",
        "  plt.ylim([0.0,1.1])\n",
        "  plt.ylabel('Recall')\n",
        "  plt.xlabel('False Positive Rate (1-Specificity)')\n",
        "  #plt.savefig(model_name+'-ROC.jpg')\n",
        "  plt.show()\n",
        "\n",
        "# Plot a heatmap of confusion matrix given the model name, a classifier model, testing data and the predicted label\n",
        "def color_confusion_matrix( model_name, model, x_test, y_test, y_predicted):\n",
        "  class_names = ['Normal', 'Intrusion']\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(6, 6))\n",
        "  plot_confusion_matrix(model, x_test, y_test, display_labels=class_names, \n",
        "                        values_format='d', ax = ax, cmap=plt.cm.Blues)\n",
        "  plt.title('Confusion Matrix of ' + str(model_name))\n",
        "  #plt.savefig(model_name+'-CM.jpg')\n",
        "  plt.show()\n",
        "\n",
        "  cmatrix = confusion_matrix(y_test, y_predicted)\n",
        "  print(cmatrix)"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1SbGMBTCCcw"
      },
      "source": [
        "# Graphing overlaid ROC curves, where each one represents a model AUC score\n",
        "def graph_multi_ROC ():\n",
        "  # Set color for each model\n",
        "  colors = {'KM': 'lightcoral','LR': 'darkorange', 'SVM':'lime', 'NB': 'steelblue',\n",
        "            'NN': 'purple','DT': 'magenta','RF': 'deeppink','KNN': 'darkturquoise',\n",
        "            'BERT': 'darkred', 'GPT': 'blue'}\n",
        "  # Set marker for each model          \n",
        "  markers = {'KM':'1--','LR': 'v--', 'SVM': '^--', 'NN': '*--', 'DT': 'o--', 'RF': '+--', 'KNN': '.--', 'NB': 'x--', 'BERT':'<--', 'GPT': '>--'}\n",
        "  \n",
        "  plt.figure(figsize=(9,6))\n",
        "  try:\n",
        "    plt.plot(KM_test.get('fpr'), KM_test.get('tpr'), markers.get('KM'), color=colors.get('KM'),  label=\"KM - AUC=\" + str(KM_test.get('auc').round(3)))\n",
        "    plt.plot(LR_test.get('fpr'), LR_test.get('tpr'), markers.get('LR'), color=colors.get('LR'),  label=\"LR - AUC=\" + str(LR_test.get('auc').round(3)))\n",
        "    plt.plot(SVM_test.get('fpr'),SVM_test.get('tpr'),markers.get('SVM'),color=colors.get('SVM'), label=\"SVM - AUC=\"+ str(SVM_test.get('auc').round(3)))\n",
        "    plt.plot(NN_test.get('fpr'), NN_test.get('tpr'), markers.get('NN'), color=colors.get('NN'),  label=\"NN - AUC=\" + str(NN_test.get('auc').round(3)))\n",
        "    plt.plot(DT_test.get('fpr'), DT_test.get('tpr'), markers.get('DT'), color=colors.get('DT'),  label=\"DT - AUC=\" + str(DT_test.get('auc').round(3)))\n",
        "    plt.plot(RF_test.get('fpr'), RF_test.get('tpr'), markers.get('RF'), color=colors.get('RF'),  label=\"RF - AUC=\" + str(RF_test.get('auc').round(3)))\n",
        "    plt.plot(KNN_test.get('fpr'),KNN_test.get('tpr'),markers.get('KNN'),color=colors.get('KNN'), label=\"KNN - AUC=\"+ str(KNN_test.get('auc').round(3)))\n",
        "    plt.plot(NB_test.get('fpr'), NB_test.get('tpr'), markers.get('NB'), color=colors.get('NB'),  label=\"NB - AUC=\" + str(NB_test.get('auc').round(3)))\n",
        "  except:\n",
        "    print('only performances of BERT and GPT are available')\n",
        "\n",
        "  plt.plot(BERT_test.get('fpr'),BERT_test.get('tpr'), markers.get('BERT'), color=colors.get('BERT'),  label=\"BERT - AUC=\"+ str(BERT_test.get('auc').round(3)))\n",
        "  plt.plot(GPT_test.get('fpr'),GPT_test.get('tpr'), markers.get('GPT'), color=colors.get('GPT'),  label=\"GPT-2 - AUC=\"+ str(GPT_test.get('auc').round(3)))\n",
        "\n",
        "  plt.plot([0,1], [0,1], 'k--', label='Random Chances')\n",
        "  plt.xlim([0.0,1.0])\n",
        "  plt.ylim([0.0,1.02])\n",
        "  plt.ylabel('Recall')\n",
        "  plt.xlabel('False Positive Rate (1-Specificity)')\n",
        "  plt.legend(loc='lower right') \n",
        "  plt.title( 'Testing ROCs on ' + DATA)\n",
        "  #plt.savefig(DATA_I+'-'+train_or_test+'.jpg', dpi = 80)\n",
        "  plt.show()"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ-JMcBqmnN9"
      },
      "source": [
        "# **BERT**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7k6L-u3Lmmpb",
        "outputId": "ae27e301-42db-4f46-9a80-c49000c6df3f"
      },
      "source": [
        "!pip install pytorch_pretrained_bert pytorch-nlp"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.7/dist-packages (0.6.2)\n",
            "Requirement already satisfied: pytorch-nlp in /usr/local/lib/python3.7/dist-packages (0.5.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (4.62.3)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.9.0+cu111)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.19.5)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.20.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.10.0.2)\n",
            "Requirement already satisfied: botocore<1.24.0,>=1.23.2 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (1.23.2)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (0.5.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.2->boto3->pytorch_pretrained_bert) (1.25.11)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.2->boto3->pytorch_pretrained_bert) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.24.0,>=1.23.2->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2021.10.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsHMmMB76B9W"
      },
      "source": [
        "from pytorch_pretrained_bert import BertModel\n",
        "from torch import nn\n",
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.optim import Adam\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_NgiJJQm6Xx"
      },
      "source": [
        "**Prepare for Train and test data for BERT**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDMTse2Jm0Bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fc1c90b-f75e-4683-fde8-47d448d90b25"
      },
      "source": [
        "train_texts = []\n",
        "for i in range(x_train.shape[0]):\n",
        "     train_texts.append(\" \".join(np.array(x_train.iloc[i,:]).astype(str)))\n",
        "train_texts = tuple(train_texts) \n",
        "\n",
        "test_texts = []\n",
        "for i in range(x_test.shape[0]):\n",
        "     test_texts.append(\" \".join(np.array(x_test.iloc[i,:]).astype(str)))\n",
        "test_texts = tuple(test_texts) \n",
        "\n",
        "train_labels = tuple(y_train.tolist())\n",
        "test_labels = tuple(y_test.tolist())\n",
        "\n",
        "len(train_texts), len(train_labels), len(test_texts), len(test_labels)"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(516, 516, 207, 207)"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gf2Lbwylm_ia",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c020a5a3-5513-4a22-8d4e-e0f2ebce8e1a"
      },
      "source": [
        "# Tokenizer \n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "# Example\n",
        "tokenizer.tokenize(train_texts[8])"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['4', '6', '91', '4', '4', '4', '4', '4', '4', '45']"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dp_XAarnZqN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4649c7f2-c82b-4609-d383-750cfac78d2a"
      },
      "source": [
        "# Convert to tokens using tokenizer\n",
        "train_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:20] + ['[SEP]'], train_texts))\n",
        "test_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:20] + ['[SEP]'], test_texts))\n",
        "\n",
        "print('Number of Training Sequences:',len(train_tokens), '\\nNumber of Testing Sequences:', len(test_tokens) )       "
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Training Sequences: 516 \n",
            "Number of Testing Sequences: 207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MH_bWrznbYv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f913974c-9d2b-4b35-b7de-c79d5f95c64a"
      },
      "source": [
        "# Following is to convert List of words to list of numbers. (Words are replaced by their index in dictionar)\n",
        "train_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, train_tokens)), maxlen=20, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
        "test_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, test_tokens)), maxlen=20, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
        "\n",
        "train_tokens_ids.shape, test_tokens_ids.shape"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((516, 20), (207, 20))"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2B9WKzfendjw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38035e6c-3682-4d53-b668-3764b2415c96"
      },
      "source": [
        "# Prepare labels\n",
        "# True if intrusion or False if normal\n",
        "train_y = np.array(train_labels) == 1\n",
        "test_y = np.array(test_labels) == 1\n",
        "train_y.shape, test_y.shape, np.mean(train_y), np.mean(test_y)"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((516,), (207,), 0.5, 0.4444444444444444)"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1q-b-LKnfbH"
      },
      "source": [
        "# To mask the paddings\n",
        "train_masks = [[float(i > 0) for i in ii] for ii in train_tokens_ids]\n",
        "test_masks = [[float(i > 0) for i in ii] for ii in test_tokens_ids]"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tZ3gKOtng_v"
      },
      "source": [
        "# Define BERT model\n",
        "class BertBinaryClassifier(nn.Module):\n",
        "    def __init__(self, dropout=0.1):\n",
        "        super(BertBinaryClassifier, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear = nn.Linear(768, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, tokens, masks=None):\n",
        "        # First Layer\n",
        "        _, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)\n",
        "\n",
        "        dropout_output = self.dropout(pooled_output)\n",
        "\n",
        "        linear_output = self.linear(dropout_output)\n",
        "        \n",
        "        # output layer\n",
        "        proba = self.sigmoid(linear_output)\n",
        "        \n",
        "        return proba\n",
        "      \n",
        "    def train_m(self,x,y,train_mask,epochs,batchsize):\n",
        "      train_tokens_tensor = torch.tensor(x)\n",
        "      train_y_tensor = torch.tensor(y.reshape(-1, 1)).float()\n",
        "      train_masks_tensor = torch.tensor(train_mask)\n",
        "\n",
        "      train_dataset = TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)\n",
        "      train_sampler = RandomSampler(train_dataset)\n",
        "      train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batchsize) \n",
        "\n",
        "\n",
        "      param_optimizer = list(self.sigmoid.named_parameters()) \n",
        "      optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
        "      optimizer = Adam(self.bert.parameters(), lr=2e-5)\n",
        "      for epoch_num in range(epochs):\n",
        "          self.train() # Training Flag\n",
        "          train_loss = 0\n",
        "          for step_num, batch_data in enumerate(train_dataloader):\n",
        "              \n",
        "              # Load batch on device memory\n",
        "              token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
        "\n",
        "              # Get the output of the model for provided input\n",
        "              logits = self(token_ids, masks)\n",
        "              \n",
        "              # Loss function\n",
        "              loss_func = nn.BCELoss()\n",
        "\n",
        "              # Calculate Loss\n",
        "              batch_loss = loss_func(logits, labels)\n",
        "              train_loss += batch_loss.item()\n",
        "              \n",
        "              # backpropagate the error\n",
        "              self.zero_grad()\n",
        "              batch_loss.backward()\n",
        "              \n",
        "              # Update the Weights of the Model\n",
        "              clip_grad_norm_(parameters=self.parameters(), max_norm=1.0)\n",
        "              optimizer.step()\n",
        "              \n",
        "              clear_output(wait=True)\n",
        "              print('Epoch: ', epoch_num + 1)\n",
        "              print(\"\\r\" + \"{0}/{1} loss: {2} \".format(step_num, len(train_labels) / batchsize, train_loss / (step_num + 1)))        "
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MA0ADfkpnjgv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3e8ba06-210d-4569-b3da-44615fdb4486"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CZERb2Onk_v"
      },
      "source": [
        "bert_clf = BertBinaryClassifier()\n",
        "bert_clf = bert_clf.cuda()"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjM08A0OoGro"
      },
      "source": [
        "**Fine Tune BERT**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUS_cifIn1_Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "6436a0cf-b966-428e-ddab-5fdc77a0393c"
      },
      "source": [
        "# Train BERT NLP\n",
        "bert_clf.train_m(train_tokens_ids,train_y,train_masks, EPOCHS, BATCH_SZ)"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-150-5f42e2323d9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train BERT NLP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbert_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_m\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tokens_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-147-75206e2d3cb6>\u001b[0m in \u001b[0;36mtrain_m\u001b[0;34m(self, x, y, train_mask, epochs, batchsize)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m               \u001b[0;31m# Get the output of the model for provided input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m               \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m               \u001b[0;31m# Loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-147-75206e2d3cb6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tokens, masks)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# First Layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_all_encoded_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mdropout_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m    731\u001b[0m         encoded_layers = self.encoder(embedding_output,\n\u001b[1;32m    732\u001b[0m                                       \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m                                       output_all_encoded_layers=output_all_encoded_layers)\n\u001b[0m\u001b[1;32m    734\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0mall_encoder_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_module\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutput_all_encoded_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0mall_encoder_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask)\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mgelu\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mAlso\u001b[0m \u001b[0msee\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0marxiv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1606.08415\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \"\"\"\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 120.00 MiB (GPU 0; 11.17 GiB total capacity; 9.90 GiB already allocated; 79.81 MiB free; 10.51 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUTlJLsooNrq"
      },
      "source": [
        "**Evaluate on Testing Set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ly5Mhn9SoKxw"
      },
      "source": [
        "# Convert token ids to tensor \n",
        "test_tokens_tensor = torch.tensor(test_tokens_ids)\n",
        "\n",
        "# Convert labels to tensors\n",
        "test_y_tensor = torch.tensor(test_y.reshape(-1, 1)).float()\n",
        "\n",
        "# Convert to tensro for maks\n",
        "test_masks_tensor = torch.tensor(test_masks)\n",
        "\n",
        "\n",
        "# Load Token, token mask and label into Dataloader\n",
        "test_dataset = TensorDataset(test_tokens_tensor, test_masks_tensor, test_y_tensor)\n",
        "\n",
        "# Define sampler\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "\n",
        "# Defile test data loader\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1gC8UusoWfo"
      },
      "source": [
        "bert_clf.eval() # Define eval\n",
        "bert_predicted = [] # To Store predicted result\n",
        "all_logits = [] # Actual output that is between 0 to 1 is stored here\n",
        "\n",
        "with torch.no_grad():\n",
        "    for step_num, batch_data in enumerate(test_dataloader):\n",
        "\n",
        "        # Load the batch on gpu memory\n",
        "        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
        "\n",
        "        # Calculate ouput of bert\n",
        "        logits = bert_clf(token_ids, masks)\n",
        "\n",
        "        # Get the numpy logits\n",
        "        numpy_logits = logits.cpu().detach().numpy()  # Detach from the GPU memory\n",
        "        \n",
        "        # Using the threshold find binary \n",
        "        bert_predicted += list(numpy_logits[:, 0] > 0.5)  # Threshold conversion\n",
        "        all_logits += list(numpy_logits[:, 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKmOvtXDoiL9"
      },
      "source": [
        "print_performance('BERT',test_y, bert_predicted)\n",
        "\n",
        "# Recording TPR and FPR for the TESTING-ROC curves\n",
        "BERT_test = {}\n",
        "BERT_test['fpr'], BERT_test['tpr'], thresh = roc_curve(test_y, bert_predicted)\n",
        "BERT_test['auc'] = roc_auc_score(test_y, bert_predicted)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxHLceizE90D"
      },
      "source": [
        "#BERT_test = {'fpr': 0.8, 'auc': 0.9}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mi1V9tJPDPxh"
      },
      "source": [
        "# Save BERT performance measures to a file\n",
        "def write_to_file (filename, varname, model_name):\n",
        "  file = open(filename, \"a\")\n",
        "  str_dictionary = repr(varname)\n",
        "  file.write(\"{}_test = \".format(model_name) + str_dictionary + \"\\n\")\n",
        "  file.close()\n",
        "\n",
        "write_to_file(\"stide-pm.txt\", BERT_test, 'BERT')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSuD_5c43X10"
      },
      "source": [
        "# **GPT-2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HueSQ1KsukV"
      },
      "source": [
        "!pip install transformers\n",
        "\n",
        "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-gD9nyo4S1D"
      },
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('microsoft/DialoGPT-small')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkYOrTFA4YUN"
      },
      "source": [
        "# Padding sequences from the right to a max length of 20\n",
        "tokenizer.padding_side = \"right\"\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "train_tokens = tokenizer(train_texts,return_tensors='pt',truncation=True,padding=True,max_length=20)\n",
        "test_tokens = tokenizer(test_texts,return_tensors='pt',truncation=True,padding=True,max_length=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHRofoti4g24"
      },
      "source": [
        "# Following is to convert List of words to list of numbers. (Words are replaced by their index in dictionar)\n",
        "\n",
        "train_tokens_ids = train_tokens.input_ids\n",
        "test_tokens_ids = test_tokens.input_ids\n",
        "\n",
        "train_tokens_ids.shape, test_tokens_ids.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udvDwEdz4haO"
      },
      "source": [
        "train_masks = train_tokens.attention_mask\n",
        "test_masks = test_tokens.attention_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23qklJA_6Q2L"
      },
      "source": [
        "**Create GPT-2 Classifer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LiNms1g6PqS"
      },
      "source": [
        "class GTP2BinaryClassifier(nn.Module):\n",
        "    def __init__(self, dropout=0.1):\n",
        "        super(GTP2BinaryClassifier, self).__init__()\n",
        "        self.gtp2 = GPT2ForSequenceClassification.from_pretrained('microsoft/DialoGPT-small')\n",
        "      \n",
        "    def train_m(self,x,y,train_mask,epochs,batchsize):\n",
        "      train_tokens_tensor = torch.tensor(x)\n",
        "      train_y_tensor = torch.tensor(y.reshape(-1, 1)).long()\n",
        "      train_masks_tensor = torch.tensor(train_mask)\n",
        "\n",
        "      train_dataset = TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)\n",
        "      train_sampler = RandomSampler(train_dataset)\n",
        "      train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batchsize) \n",
        "\n",
        "\n",
        "      # param_optimizer = list(self.gtp2.parameters()) \n",
        "      # optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
        "      optimizer = Adam(self.gtp2.parameters(), lr=5e-5)\n",
        "      for epoch_num in range(epochs):\n",
        "          self.gtp2.train() # Training Flag\n",
        "          train_loss = 0\n",
        "          for step_num, batch_data in enumerate(train_dataloader):\n",
        "              \n",
        "              # Load batch on device memory\n",
        "              token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
        "              self.zero_grad()\n",
        "\n",
        "              # Get the output of the model for provided input\n",
        "              outputs = self.gtp2(token_ids,attention_mask=masks,labels=labels)\n",
        "              loss, logits = outputs[:2]\n",
        "              # logits = self(token_ids, masks)\n",
        "              \n",
        "              # Total Loss\n",
        "              train_loss += loss.item()\n",
        "              \n",
        "              # Backward pass the loss\n",
        "              loss.backward()\n",
        "              torch.nn.utils.clip_grad_norm_(self.gtp2.parameters(), 1.0)\n",
        "              \n",
        "              optimizer.step()\n",
        "              logits = logits.detach().cpu().numpy()\n",
        "\n",
        "              clear_output(wait=True)\n",
        "        \n",
        "              print('Epoch: ', epoch_num + 1)\n",
        "              print(\"\\r\" + \"{0}/{1} loss: {2} \".format(step_num, len(train_labels) / batchsize, train_loss / (step_num + 1)))\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPZ0MZU06Xxu"
      },
      "source": [
        "gtp_clf = GTP2BinaryClassifier()\n",
        "gtp_clf = gtp_clf.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uc6BBhJ_6gQL"
      },
      "source": [
        "# Configure the Padding token id\n",
        "gtp_clf.gtp2.config.pad_token_id = tokenizer.eos_token_id\n",
        "gtp_clf.train_m(train_tokens_ids,train_y,train_masks, EPOCHS, BATCH_SZ)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIQXu7cL6n1g"
      },
      "source": [
        "**Evaluate on Testing Set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpBPha-z6gyA"
      },
      "source": [
        "test_tokens_tensor = torch.tensor(test_tokens_ids)\n",
        "test_y_tensor = torch.tensor(test_y.reshape(-1, 1)).long()\n",
        "\n",
        "test_masks_tensor = torch.tensor(test_masks)\n",
        "\n",
        "\n",
        "test_dataset = TensorDataset(test_tokens_tensor, test_masks_tensor, test_y_tensor)\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=16)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QSujIkh6s79"
      },
      "source": [
        "# Evaluate Model\n",
        "gtp_clf.eval() # Define eval\n",
        "gpt_predicted = [] # Store Result\n",
        "with torch.no_grad():\n",
        "    for step_num, batch_data in enumerate(test_dataloader):\n",
        "\n",
        "        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
        "\n",
        "        # ----------------------------------------------------------------\n",
        "        outputs = gtp_clf.gtp2(token_ids,attention_mask=masks,labels=labels)\n",
        "        loss, logits = outputs[:2]\n",
        "        numpy_logits = logits.detach().cpu().numpy()\n",
        "        # ----------------------------------------------------------------\n",
        "        gpt_predicted +=list(numpy_logits.argmax(axis=-1).flatten().tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ke386Aoj6xXG"
      },
      "source": [
        "print_performance('GPT2',test_y, gpt_predicted)\n",
        "\n",
        "# Recording TPR and FPR for the TESTING-ROC curves\n",
        "GPT_test = {}\n",
        "GPT_test['fpr'], GPT_test['tpr'], thresh = roc_curve(test_y, gpt_predicted)\n",
        "GPT_test['auc'] = roc_auc_score(test_y, gpt_predicted) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5t8JqbTG1qm"
      },
      "source": [
        "write_to_file(\"stide-pm.txt\", GPT_test, 'GPT')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJgQwUXi62OA"
      },
      "source": [
        "# Ploting the overlaid ROC curves on testing results:\n",
        "graph_multi_ROC()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-KVm3qLDNsk"
      },
      "source": [
        "# **Write performance measures to file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CIEcZ4ke9k1"
      },
      "source": [
        "# Save performance measure dict of each model to a file\n",
        "def write_to_file (varname, model_name):\n",
        "  clean_status = 'clean' if CLEAN else 'unclean'\n",
        "  filename = DATA +'-'+ str(SEQ_WINDOW) +'-'+ clean_status + \".txt\"\n",
        "  file = open(filename, \"a\")\n",
        "  str_dictionary = repr(varname)\n",
        "  file.write(\"{}_test = \".format(model_name) + str_dictionary + \"\\n\")\n",
        "  file.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uusevnMCe9kO"
      },
      "source": [
        "write_to_file(BERT_test, 'BERT')\n",
        "write_to_file(GPT_test, 'GPT')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}