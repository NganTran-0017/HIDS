{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HIDS-Data-Processing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNgkCDdMILXUcr+tGT5WEh6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NganTran-0017/HIDS/blob/main/HIDS_Data_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "EHqQTqWhvpYB"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Update remove duplicate between train and test --> create independent test\n",
        "check why test set after removing the intersection (aka independent test) has no frequent record? --> no, set(independent test) has fewer records than independent test.\n",
        "So the previous clean_data method removes duplication and overlap instances, while the new method (remove_duplicate) only removes overlap instance\n"
      ],
      "metadata": {
        "id": "o_IP76v1DNX1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#@title Specify parameters before running\n",
        "\n",
        "\n",
        "SZ =  1#@param {type:\"number\"}         # Indicate a fraction number to sample train set when it's too big. Located in Data Partition\n",
        "\n",
        "SEQ_WINDOW =  25#@param {type:\"integer\"} # Indicate the window length to parse the sequence into. Used in Data Parsing section\n",
        "\n",
        "BATCH_SZ =  32#@param {type:\"integer\"} # Indicate the window length to parse the sequence into. Used in Data Parsing section\n",
        "\n",
        "EPOCHS =  2#@param {type:\"integer\"} # Indicate the window length to parse the sequence into. Used in Data Parsing section\n",
        "\n",
        "# Indicate to clean data or not. Used in Data Cleaning section\n",
        "CLEAN = True #@param {type:\"boolean\"}\n"
      ],
      "metadata": {
        "id": "baLX852RsM7a"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDDU0UkRplxK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b1645e8-d4ab-47d1-b778-0580084dde09"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, accuracy_score, roc_curve, auc, recall_score, precision_score,plot_confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "%matplotlib inline\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"popular\")\n",
        "\n",
        "## Tokenizing syscall sequences into n-grams of 6\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import ngrams"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4SwCY4NVnCL"
      },
      "source": [
        "#**Processing data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQhzBhHanZuz"
      },
      "source": [
        "Use the given datasets in our GitHub to load the data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ADFA-LD**"
      ],
      "metadata": {
        "id": "c4QQ_y7Iwr1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load ADFA-LD and unzip them\n",
        "! wget 'https://github.com/NganTran-0017/HIDS/blob/main/Datasets/ADFA-LD/Training_Data_Master.zip?raw=true'\n",
        "! wget 'https://github.com/NganTran-0017/HIDS/blob/main/Datasets/ADFA-LD/Attack_Data_Master.zip?raw=true'\n",
        "# Rename files\n",
        "! mv Attack_Data_Master.zip?raw=true   Attack_Data_Master.zip\n",
        "! mv Training_Data_Master.zip?raw=true Training_Data_Master.zip\n",
        "\n",
        "# Unzip files\n",
        "! unzip Training_Data_Master.zip\n",
        "! unzip Attack_Data_Master.zip\n",
        "\n",
        "from glob import glob\n",
        "import os\n",
        "train_dir = 'Training_Data_Master/'\n",
        "test_dir  = 'Attack_Data_Master/'\n",
        "\n",
        "# Read data in a dataframe\n",
        "def read_in_data(path, is_normal = 1): \n",
        "  lines = []\n",
        "  if is_normal == 1:\n",
        "    for filename in os.listdir(path): # iterate through all label folders in a directory \n",
        "      with open(path+filename) as f:\n",
        "        lines.append(f.read())\n",
        "       # data = pd.read_csv(path+filename, sep=' ', header=None)\n",
        "       # list_of_dataframes.append(data.stack())\n",
        "    \n",
        "  else:\n",
        "    for folder in os.listdir(path): # iterate through different attack folders in a directory\n",
        "      files = glob(str(path + folder +\"/*.txt\"), recursive=False) # get a list of files from each label folder \n",
        "      for filename in files: \n",
        "        with open(filename) as f:\n",
        "          lines.append(f.read())\n",
        "        #data = pd.read_csv(filename, sep=' ', header=None)\n",
        "        #lines.append(data.stack())\n",
        "\n",
        "  df = pd.DataFrame(lines)\n",
        "  return df   \n",
        "\n",
        "\n",
        "# Trainning set\n",
        "df = read_in_data(train_dir)\n",
        "df.rename(columns={0:'Syscall Sequence'}, inplace=True)\n",
        "\n",
        "print('Normal training data size:', df.shape)\n",
        "print(df.head(5))\n",
        "\n",
        "# Testing set\n",
        "intrusiondf = read_in_data(test_dir, is_normal = 0)\n",
        "intrusiondf.rename(columns={0:'Syscall Sequence'}, inplace=True)\n",
        "\n",
        "print('Intrusion Testing data size:', intrusiondf.shape)\n",
        "intrusiondf.head(5)"
      ],
      "metadata": {
        "id": "LZf5mfR7wfZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **UNM and MIT Lpr**"
      ],
      "metadata": {
        "id": "Bc0RHMXQwufW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "JHtiWDffY_so",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63a76488-27a0-4646-cd4c-2c22e90b9ec1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-04 20:47:32--  https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/4.%20Live%20lpr/lpr-normal-all.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5575954 (5.3M) [text/plain]\n",
            "Saving to: ‘lpr-normal-all.txt’\n",
            "\n",
            "lpr-normal-all.txt  100%[===================>]   5.32M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-04-04 20:47:33 (54.4 MB/s) - ‘lpr-normal-all.txt’ saved [5575954/5575954]\n",
            "\n",
            "Normal data size: (553336, 2)\n",
            "--2022-04-04 20:47:36--  https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/4.%20Live%20lpr/Intrusion/exploit-unm.int\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1829578 (1.7M) [text/plain]\n",
            "Saving to: ‘exploit-unm.int’\n",
            "\n",
            "exploit-unm.int     100%[===================>]   1.74M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2022-04-04 20:47:36 (25.1 MB/s) - ‘exploit-unm.int’ saved [1829578/1829578]\n",
            "\n",
            "intrusion data size:  (164232, 2)\n",
            "Normal:      PID  Syscall\n",
            "0  20301        4\n",
            "1  20301        2\n",
            "2  20301       66\n",
            "Intrusion:      PID  Syscall\n",
            "0  22799        4\n",
            "1  22799        2\n",
            "2  22799       66\n",
            "3  22799       66\n",
            "4  22799        4\n"
          ]
        }
      ],
      "source": [
        "  ## Uncomment each line to load Normal data\n",
        "\n",
        "# Synthetic sendmail csv_file = ['Sendmail-normal.txt']; DATA = 'Synthetic Sendmail'; DATA_I='Synthetic_Sendmail'\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/1.Synthetic%20Sendmail/Sendmail-normal.txt'\n",
        "\n",
        "# LIVE LPR \n",
        "csv_file = ['lpr-normal-all.txt']; DATA = 'Live Lpr'; DATA_I='Live-Lpr' \n",
        "!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/4.%20Live%20lpr/lpr-normal-all.txt'\n",
        "\n",
        "# MIT live lpr csv_file = [ 'MIT-lpr-normal-all.txt']; DATA = 'MIT Live Lpr'; DATA_I='MIT-Lpr' \n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/MIT/MIT-lpr-normal-all.txt'\n",
        "\n",
        "# LOGIN and PS csv_file = [ 'login-normal.txt', 'ps-normal.txt']; DATA = 'Login and Ps'; DATA_I =\"Login-and-Ps\"\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/7.Login_and_ps/normal/login-normal.txt'\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/7.Login_and_ps/normal/ps-normal.txt'\n",
        "\n",
        "# INETD csv_file = [ 'inetd-live-unm.int']; DATA = 'Inetd';DATA_I =\"Inetd\" \n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/8.Inetd/inetd-live-unm.int'\n",
        "\n",
        "# STIDE csv_file = [ 'stide-normal.txt']; DATA = 'Stide';DATA_I ='Stide' \n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/10.Stide/stide-normal.txt.gz'\n",
        "#!gunzip 'stide-normal.txt.gz'\n",
        "\n",
        "# Live Named csv_file = [ 'named-live-unm']; DATA = 'Live Named';DATA_I ='Live-Named' \n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/6.Live_named/named-live-unm'\n",
        "\n",
        "# Xlock csv_file = [ 'normal-xlock.txt']; DATA = 'Xlock';DATA_I='Xlock' \n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/5.xlock/normal-xlock.txt'\n",
        "\n",
        "# Synthetic Ftp \n",
        "#csv_file = [ 'nonself1.int','nonself2.int']; DATA = 'Synthetic Ftp'; DATA_I='Synthetic-Ftp'\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/2.Synthetic%20Ftp/nonself1.int'\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/2.Synthetic%20Ftp/nonself2.int'\n",
        "\n",
        "# Synthetic lpr csv_file = ['syn.int']; DATA = 'Synthetic Lpr';DATA_I='Synthetic-Lpr'\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/3.Synthetic-lpr/syn.int'\n",
        "\n",
        "# Concat a list of files into normal df\n",
        "list_of_dataframes = []\n",
        "for filename in csv_file:\n",
        "    list_of_dataframes.append(pd.read_csv(filename, sep=' ', header=None, engine='python'))\n",
        "df = pd.concat(list_of_dataframes)\n",
        "\n",
        "# Check number of columns, if > 2, then drop the excess\n",
        "if len(df.columns) > 2:\n",
        "    df=df.drop(labels=None, axis=1, columns = [2,3])\n",
        "df =df.rename(columns= {0:\"PID\", 1:\"Syscall\"})\n",
        "\n",
        "print('Normal data size:', df.shape)\n",
        "\n",
        "\n",
        "  ## Uncomment each line to load Intrusion data:\n",
        "\n",
        "# Synthetic sendmail csv_file = ['Sendmail-intrusion.txt']\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/1.Synthetic%20Sendmail/intrusion/Sendmail-intrusion.txt'\n",
        "\n",
        "# LIVE LPR \n",
        "csv_file =['exploit-unm.int'] \n",
        "!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/4.%20Live%20lpr/Intrusion/exploit-unm.int'\n",
        "\n",
        "# MIT live lpr csv_file = [ 'exploit-ai.int'] \n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/MIT/Intrusion/exploit-ai.int'\n",
        "\n",
        "# LOGIN and PS csv_file = [ 'login-homegrown.int','ps-homegrown.int','login-recovered.int','ps-recovered.int']\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/7.Login_and_ps/intrusion/ps-recovered.int'\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/7.Login_and_ps/intrusion/ps-homegrown.int'\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/7.Login_and_ps/intrusion/login-recovered.int'\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/7.Login_and_ps/intrusion/login-homegrown.int'\n",
        "\n",
        "# INETD csv_file = [ 'inetd-intrusion.int'] \n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/8.Inetd/intrusion/inetd-intrusion.int'\n",
        "\n",
        "# STIDE csv_file = [ 'stide-intrusion'] \n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/10.Stide/intrusion/stide-intrusion'\n",
        "\n",
        "# Live Named  csv_file = [ 'exploit-1.int','exploit-2.int'] \n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/6.Live_named/intrusion/exploit-1.int'\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/6.Live_named/intrusion/exploit-2.int'\n",
        "\n",
        "# Xlock csv_file = [ 'nonself.cs.unm.edu-07.24.97-xlock-2822_new.log.int', 'nonself.cs.unm.edu-07.25.97-xlock-2691_new.log.int']\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/5.xlock/intrusion/nonself.cs.unm.edu-07.25.97-xlock-2691_new.log.int'\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/5.xlock/intrusion/nonself.cs.unm.edu-07.24.97-xlock-2822_new.log.int'\n",
        "\n",
        "# Synthetic Ftp csv_file = [ 'exploit2.int']\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/2.Synthetic%20Ftp/intrusion/exploit2.int'\n",
        "\n",
        "# Synthetic Lpr csv_file = [ 'exploit-unm.int']\n",
        "#!wget 'https://raw.githubusercontent.com/NganTran-0017/HIDS/main/Datasets/UNM/3.Synthetic-lpr/intrusion/exploit-unm.int'\n",
        "\n",
        "list_of_dataframes = []\n",
        "for filename in csv_file:\n",
        "    list_of_dataframes.append(pd.read_csv(filename, sep=' ', header=None, engine='python'))\n",
        "intrusiondf = pd.concat(list_of_dataframes)\n",
        "\n",
        "if len(intrusiondf.columns) > 2:\n",
        "    intrusiondf = intrusiondf.drop(labels=None, axis=1, columns = [2,3])\n",
        "intrusiondf = intrusiondf.rename(columns= {0:\"PID\", 1:\"Syscall\"})\n",
        "\n",
        "print('intrusion data size: ', intrusiondf.shape)\n",
        "\n",
        "print('Normal:',df.head(3))\n",
        "#print(df['PID'].value_counts())\n",
        "print('Intrusion:',intrusiondf.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09eozmwq9CFh"
      },
      "source": [
        "**Change to covert all syscall of 1 PID into a data record. Pasrse each data record to a length of 10 or 15, clean frequent records.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape, intrusiondf.shape"
      ],
      "metadata": {
        "id": "D2u4b-VTPJfb",
        "outputId": "cfe0ff67-151d-46b4-a5b1-bafb814704cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((553336, 2), (164232, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvHyIKtq9BCG"
      },
      "source": [
        "**Create syscall sequence per pid**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXOlXVgyOHmA",
        "outputId": "182af413-f985-48dd-9b93-7dcad453d5c8"
      },
      "source": [
        "# This function groups data by PID, so the sequences appear by PID instead of by order, in case it was interrupted by other PID\n",
        "## It returns a dict with PID as key and syscall seq as item\n",
        "def group_syscalls_by_pid (data):\n",
        "  seq_per_pid = {}\n",
        "  for p in data['PID'].unique():\n",
        "    filt = data['PID'] == p\n",
        "    seq = data.loc[filt]['Syscall'].values.astype(str)\n",
        "    seq_per_pid[p] = ' '.join(seq)\n",
        "  return seq_per_pid\n",
        "\n",
        "if df.shape[1] == 2:\n",
        "  print('Number of unique PID in normal data:', len(df['PID'].value_counts()))\n",
        "  print('Number of unique PID in intrusion data:', len(intrusiondf['PID'].value_counts()))\n",
        "\n",
        "  # Group normal df by PID and drop PID column\n",
        "  normal_seq_per_pid = group_syscalls_by_pid(df)\n",
        "  print('Number of unique PID in normal:', len(normal_seq_per_pid))\n",
        "  #print('Normal PIDs and its sequences: ',normal_seq_per_pid)\n",
        "\n",
        "  # Do the same thing to intrusion PID\n",
        "  intrusion_seq_per_pid = group_syscalls_by_pid(intrusiondf)\n",
        "  print('Number of unique PID in intrusion:', len(intrusion_seq_per_pid))\n",
        "  #print('Intrusion PIDs and its sequences: ', intrusion_seq_per_pid)\n",
        "\n",
        "else:\n",
        "  print('ADFA-LD data does not have PID column')"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique PID in normal data: 1201\n",
            "Number of unique PID in intrusion data: 1001\n",
            "Number of unique PID in normal: 1201\n",
            "Number of unique PID in intrusion: 1001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3M7Z4L6modm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caa2ed61-daad-41dc-f8db-3dde580180f9"
      },
      "source": [
        "# Drop a sequence if its total len is less than 3\n",
        "def remove_small_seq(pid_seq_dict):\n",
        "  removed_pid = []\n",
        "  for pid in pid_seq_dict:\n",
        "    seq_list = pid_seq_dict[pid].split()\n",
        "    if len(seq_list) < 3:\n",
        "      print('Remove PID %d which only has %d syscals in its sequence: %s' % (pid, len(seq_list), pid_seq_dict[pid]))\n",
        "      removed_pid.append(pid)\n",
        "\n",
        "  [pid_seq_dict.pop(pid) for pid in removed_pid]\n",
        "  return pid_seq_dict\n",
        "\n",
        "if df.shape[1] ==2:\n",
        "  # Clean small intrusion sequences\n",
        "  print('Clean small normal seq: \\nNum PID in Normal before:', len(normal_seq_per_pid))\n",
        "  normal_seq_per_pid = remove_small_seq(normal_seq_per_pid)\n",
        "  df = pd.DataFrame.from_dict(normal_seq_per_pid, orient = 'index', columns=['Syscall Sequence'] )\n",
        "  print('Num PID in Normal after:', len(normal_seq_per_pid))\n",
        "\n",
        "  # Clean small intrusion sequences\n",
        "  print('\\n\\nClean small intrusion seq: \\nNum PID in Intrusion before:', len(intrusion_seq_per_pid))\n",
        "  intrusion_seq_per_pid = remove_small_seq(intrusion_seq_per_pid)\n",
        "  intrusiondf = pd.DataFrame.from_dict(intrusion_seq_per_pid, orient = 'index', columns=['Syscall Sequence'] )\n",
        "  print('Num PID in Intrusion after:', len(intrusion_seq_per_pid))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean small normal seq: \n",
            "Num PID in Normal before: 1201\n",
            "Num PID in Normal after: 1201\n",
            "\n",
            "\n",
            "Clean small intrusion seq: \n",
            "Num PID in Intrusion before: 1001\n",
            "Num PID in Intrusion after: 1001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKXCVvBRF9fB"
      },
      "source": [
        "## **Data Parsing**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "GK4lOjsyyyq_",
        "outputId": "013bfe78-0c5e-416e-de0e-a785f7f6b4d5"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                        Syscall Sequence\n",
              "20301  4 2 66 66 4 138 66 5 23 45 4 27 66 5 4 2 66 66...\n",
              "20709  4 2 66 66 4 138 66 5 23 45 4 27 66 5 4 2 66 66...\n",
              "20722  4 2 66 66 4 138 66 5 23 45 4 27 66 5 4 2 66 66...\n",
              "20728  4 2 66 66 4 138 66 5 23 45 4 27 66 5 4 2 66 66...\n",
              "20734  4 2 66 66 4 138 66 5 23 45 4 27 66 5 4 2 66 66...\n",
              "...                                                  ...\n",
              "28007  4 2 66 66 4 138 66 5 23 45 4 27 66 5 4 2 66 66...\n",
              "28014  4 2 66 66 4 138 66 5 23 45 4 27 66 5 4 2 66 66...\n",
              "28148  4 2 66 66 4 138 66 5 23 45 4 27 66 5 4 2 66 66...\n",
              "28156  4 2 66 66 4 138 66 5 23 45 4 27 66 5 4 2 66 66...\n",
              "28187  4 2 66 66 4 138 66 5 23 45 4 27 66 5 4 2 66 66...\n",
              "\n",
              "[1201 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-805cef63-7d40-40f9-99bd-c4d69748ea85\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Syscall Sequence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>20301</th>\n",
              "      <td>4 2 66 66 4 138 66 5 23 45 4 27 66 5 4 2 66 66...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20709</th>\n",
              "      <td>4 2 66 66 4 138 66 5 23 45 4 27 66 5 4 2 66 66...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20722</th>\n",
              "      <td>4 2 66 66 4 138 66 5 23 45 4 27 66 5 4 2 66 66...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20728</th>\n",
              "      <td>4 2 66 66 4 138 66 5 23 45 4 27 66 5 4 2 66 66...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20734</th>\n",
              "      <td>4 2 66 66 4 138 66 5 23 45 4 27 66 5 4 2 66 66...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28007</th>\n",
              "      <td>4 2 66 66 4 138 66 5 23 45 4 27 66 5 4 2 66 66...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28014</th>\n",
              "      <td>4 2 66 66 4 138 66 5 23 45 4 27 66 5 4 2 66 66...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28148</th>\n",
              "      <td>4 2 66 66 4 138 66 5 23 45 4 27 66 5 4 2 66 66...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28156</th>\n",
              "      <td>4 2 66 66 4 138 66 5 23 45 4 27 66 5 4 2 66 66...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28187</th>\n",
              "      <td>4 2 66 66 4 138 66 5 23 45 4 27 66 5 4 2 66 66...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1201 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-805cef63-7d40-40f9-99bd-c4d69748ea85')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-805cef63-7d40-40f9-99bd-c4d69748ea85 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-805cef63-7d40-40f9-99bd-c4d69748ea85');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ph244VM5_86n"
      },
      "source": [
        "## Parse an entire Syscall seq per PID into smaller sequences of size 15\n",
        "def parse_seq(seq_per_pid):\n",
        "  sequences = pd.DataFrame()\n",
        "  for _, row in seq_per_pid.iterrows():\n",
        "    token = word_tokenize(row['Syscall Sequence'])  # Tokenize the string of sequence\n",
        " \n",
        "    # Parse the sequence into length of 15\n",
        "    sequences=sequences.append(list(nltk.ngrams(token, 25, pad_right=True, right_pad_symbol=-1)))\n",
        "    #print('PID %d - seq len: %d'% (p, len(sequences)))\n",
        "  return sequences\n",
        "\n",
        "#normal = parse_seq(normal_seq_per_pid)\n",
        "normal = parse_seq(df)\n",
        "normal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyAnefsOQiIU"
      },
      "source": [
        "print('Parsing Intrusion')\n",
        "intrusion = parse_seq(intrusiondf)\n",
        "intrusion"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WjkWBLbqW3u"
      },
      "source": [
        "Start tokenizing system calls into n-grams as specified in the params at the top of the page."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Preparing data for data partition**\n",
        "- Combine data and labels together. \n",
        "- Combine data from each class together --> Create a pool of data \n",
        "- Split data pool into train and test\n",
        "\n",
        "## **Cleaning**\n",
        "- Remove dupplication between train and test\n",
        "- Save train set. Save test set as unclean test\n",
        "- Clean test set and save as clean test \n",
        "\n",
        "- 2 directions for Testing set:\n",
        "  *  Test with clean data: Remove overlap and dupplication between Normal and Intrusion in Test data\n",
        "  *  Test with Unclean data: Leave Test data as is\n",
        "\n",
        "- 2 directions Training set:\n",
        "  *   Clean model: Remove overlap and dupplication between Normal and Intrusion in Train data\n",
        "  *   Unclean model: Leave Train data as is\n",
        "\n"
      ],
      "metadata": {
        "id": "Y-0vzrcTekD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Add Label to Data**"
      ],
      "metadata": {
        "id": "Yi1Zi_gjebMG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Zbd4WjumD8S"
      },
      "source": [
        "print('Normal len:',len(normal),'\\nIntrusion len:', len(intrusion))\n",
        "normal['Label'] = 0     # Normal sequences is labeled 0.\n",
        "intrusion['Label'] = 1  # Intrusion sequences is labeled 1."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNQcvhiImD8S"
      },
      "source": [
        "## **Partition Training and Testing dataset 70/30**\n",
        "- Combine both classes together then split data.\n",
        "- We do not need to bootstrap since there are more than enough data from both classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bg1ZNmR7mD8T"
      },
      "source": [
        "# combine normal(train) and intrusion(test) data and split them into training and testing sets\n",
        "df = normal.append(intrusion, ignore_index=True).astype(int)\n",
        "print('Df sz:', df.shape)\n",
        "\n",
        "# Spliting into training and testing\n",
        "x_train, x_test, y_train, y_test = train_test_split(df, df['Label'], test_size = 0.30, shuffle=True, stratify=df['Label'])\n",
        "\n",
        "# Reset index of training and testing sets\n",
        "x_train.reset_index(drop=True, inplace=True); y_train.reset_index(drop=True, inplace=True)\n",
        "x_test.reset_index(drop=True, inplace=True);  y_test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "print('Train sz:',len(x_train), len(y_train))\n",
        "intrusion_train = y_train.loc[y_train == 1]\n",
        "normal_train = y_train.loc[y_train == 0]\n",
        "print('Train set: Intrusion vs. Normal cases', len(y_train.iloc[intrusion_train] ), len(y_train.iloc[normal_train] ))\n",
        "\n",
        "print('Test sz:', len(x_test), len(y_test))\n",
        "intrusion_test = y_test.loc[y_test == 1]\n",
        "normal_test = y_test.loc[y_test == 0]\n",
        "print('Test set: Intrusion vs. Normal cases', len(y_test.iloc[intrusion_test] ), len(y_test.iloc[normal_test] ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Cleaning**\n",
        "**Remove Overlap between Train and Test**\n",
        "--> Ensures reliability in model performance\n",
        "\n",
        "**Remove Dupplicate or Frequent Data in Test\"\"\n",
        "--> Ensures a fair evaluation because if Test has a lot of dupplication, model will either classify all dupplication correctly or incorrectly --> Not fair and not reliable in evaluating model performance "
      ],
      "metadata": {
        "id": "5fLGd0NIigs5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Cleaning testing data**"
      ],
      "metadata": {
        "id": "xHh1jy7ISKW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Get % of duplicates in both datasets\n",
        "\n",
        "# Convert normal df to set, and intrusion df to set\n",
        "def clean_data(normal, intrusion):\n",
        "  normal_list = normal.values.tolist()\n",
        "  intrusion_list = intrusion.values.tolist()\n",
        "  normal_set = set(tuple(i) for i in normal_list)\n",
        "  intrusion_set = set(tuple(i) for i in intrusion_list)\n",
        "  print('List sz vs. Set sz of normal sequences: %d vs. %d'% (len(normal_list),len(normal_set)) )\n",
        "  print('List sz vs. Set sz of intrusion sequences: %d vs. %d'% (len(intrusion_list),len(intrusion_set)) )\n",
        "    \n",
        "  normal_dupplication = (len(normal_list) - len(normal_set)) /len(normal_list)*100 \n",
        "  intrusion_duplication = (len(intrusion_list)-len(intrusion_set))/len(intrusion_list) * 100\n",
        "\n",
        "  print('Duplication Rate in Normal Class: %.3f%%'% normal_dupplication )\n",
        "  print('Duplication Rate in Intrusion Class: %.3f%%'% intrusion_duplication) \n",
        " \n",
        "  c_intrusion = intrusion_set - normal_set \n",
        "  overlap_rate =  len(normal_set.intersection(intrusion_set)) / len(normal_set.union(intrusion_set)) * 100\n",
        "  print('Overlap rate: %.3f%%' % overlap_rate)\n",
        "  \n",
        "  #c_normal = normal_set - intrusion_set\n",
        "  if len(c_intrusion) == 0:\n",
        "    print(DATA+' No Duplication!')\n",
        "  if len(c_intrusion) > 0:\n",
        "    intrusion = pd.DataFrame(c_intrusion)\n",
        "  else:\n",
        "    intrusion = pd.DataFrame(intrusion_set)\n",
        "  #if len(c_normal) > 0:\n",
        "  #  normal = pd.DataFrame(c_normal)\n",
        "  #else:\n",
        "  normal = pd.DataFrame(normal_set)\n",
        "\n",
        "  print('After cleaning: \\nNormal sz:', len(normal), ' Intrusion sz:', len(c_intrusion) )\n",
        "  return normal, intrusion\n",
        "\n",
        "\n",
        "def remove_overlap2(train, test):\n",
        "  train_list = train.values.tolist()\n",
        "  test_list = test.values.tolist()\n",
        "  train_set = set(tuple(i) for i in train_list)\n",
        "  test_set = set(tuple(i) for i in test_list)\n",
        "  print('List sz vs. Set sz of training sequences: %d vs. %d'% (len(train_list),len(train_set)) )\n",
        "  print('List sz vs. Set sz of testing sequences: %d vs. %d'% (len(test_list),len(test_set)) )\n",
        "    \n",
        "  train_dupplication  = (len(train_list) - len(train_set)) /len(train_list)*100 \n",
        "  test_duplication    = (len(test_list) - len(test_set))/len(test_list) * 100\n",
        "\n",
        "  print('Duplication Rate in training set: %.3f%%'% train_dupplication )\n",
        "  print('Duplication Rate in test set: %.3f%%'% test_duplication) \n",
        "\n",
        "  intersect = train_set.intersection(test_set)\n",
        "  overlap_rate = len(intersect)*100/(len(train_set.union(test_set)))\n",
        "  print( 'Overlap rate is %.3f%%'% overlap_rate)\n",
        "  intersection_df = pd.DataFrame.from_dict(intersect).rename(columns={25:'Label'})\n",
        "  independent_test = pd.merge(intersection_df, test, how = 'outer', indicator = True).query('_merge==\"right_only\"').drop(columns = '_merge')\n",
        "\n",
        "  return independent_test"
      ],
      "metadata": {
        "id": "0RyIRcT_idT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "independent_test = remove_overlap2(x_train, x_test)\n",
        "independent_test"
      ],
      "metadata": {
        "id": "b0jca1egvMIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test"
      ],
      "metadata": {
        "id": "TQNmLn6k849o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check how much data have left after condensing it as a set\n",
        "a = independent_test.values.tolist()\n",
        "a_set = set(tuple(i) for i in a)\n",
        "len(a_set)"
      ],
      "metadata": {
        "id": "6L7H228-6lHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "left = pd.DataFrame({'ID': [1,2,3,1,4], 'name': ['Ngan', 'Juan', 'Dave', 'Ngan', 'JD']})\n",
        "right = pd.DataFrame({'ID': [2,2,4], 'name': ['Natti', 'Natti', 'JD']})\n",
        "\n",
        "pd.merge(left, right, how='outer', indicator=True).query('_merge==\"left_only\"')"
      ],
      "metadata": {
        "id": "QHwgeLCu9imX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fx2FIGucl8E"
      },
      "source": [
        "# Separate normal and intrusion in Test Clean so that I can call func clean_data on them\n",
        "def separate_two_classes (data):\n",
        "  ## Filter normal data from Test and drop label column\n",
        "  filt = data.loc[:, 'Label'] == 0\n",
        "  normal_class = data.loc[filt]\n",
        "  normal_class.drop(columns = 'Label', inplace = True)\n",
        "\n",
        "  ## Filter Intrusion data from Test and drop label column\n",
        "  intrusion_class = data.loc[~filt]\n",
        "  intrusion_class.drop(columns = 'Label', inplace = True)\n",
        "  return normal_class, intrusion_class\n",
        "\n",
        "x_test = independent_test.copy()\n",
        "test_normal, test_intrusion = separate_two_classes(x_test)\n",
        "print('Before cleaning:', x_test.shape)\n",
        "test_clean_normal, test_clean_intrusion = clean_data(test_normal, test_intrusion)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add label back to data\n",
        "test_clean_normal['Label'] = 0\n",
        "test_clean_intrusion['Label'] = 1\n",
        "test_clean = test_clean_normal.append(test_clean_intrusion, ignore_index= True)\n",
        "test_clean = test_clean.sample(frac=1).reset_index(drop=True) #Shuffle data\n",
        "test_clean"
      ],
      "metadata": {
        "id": "JLyqa8ZVyiw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_clean.shape, x_test.shape"
      ],
      "metadata": {
        "id": "dXihAWueLydC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Cleaning training data**"
      ],
      "metadata": {
        "id": "KQgDnn9pSlg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_normal, train_intrusion = separate_two_classes(x_train)\n",
        "print('Before cleaning:', x_train.shape)\n",
        "train_clean_normal, train_clean_intrusion = clean_data(train_normal, train_intrusion)\n",
        "\n",
        "# Add label back to data\n",
        "train_clean_normal['Label'] = 0\n",
        "train_clean_intrusion['Label'] = 1\n",
        "train_clean = pd.concat([train_clean_normal, train_clean_intrusion], ignore_index=True) #test_clean_normal.append(test_clean_intrusion, ignore_index= True)\n",
        "train_clean = train_clean.sample(frac=1).reset_index(drop=True) #Shuffle data\n",
        "print('After cleaning:', len(train_clean))"
      ],
      "metadata": {
        "id": "E0wblCMnSj6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.to_csv('train.csv', index=False)\n",
        "train_clean.to_csv('train_clean.csv', index=False)\n",
        "test_clean.to_csv('test_clean.csv', index=False)\n",
        "x_test.to_csv('test_unclean.csv', index=False)"
      ],
      "metadata": {
        "id": "xiy_l4rzzaKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.to_csv(\"train.csv.gz\", index=False, compression=\"gzip\")"
      ],
      "metadata": {
        "id": "T0O2g_7rTZQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5Ox50YqgVA5a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}