{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL-Models-HIDS.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "8GMYBPNarqFf",
        "-0UCA-ZuXHX_"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NganTran-0017/HIDS/blob/main/DL_Models_HIDS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Mzc_frRBqSW"
      },
      "source": [
        "#Bootstrapped Live Lpr trained with Clean model"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baLX852RsM7a"
      },
      "source": [
        "#@title Specify parameters before running\n",
        "\n",
        "\n",
        "SZ =  .3#@param {type:\"number\"}         # Indicate a fraction number to sample train set when it's too big. Located in Data Partition\n",
        "\n",
        "SEQ_WINDOW =  25#@param {type:\"integer\"} # Indicate the window length to parse the sequence into. Used in Data Parsing section\n",
        "\n",
        "BATCH_SZ =  128#@param {type:\"integer\"} # Indicate the window length to parse the sequence into. Used in Data Parsing section\n",
        "\n",
        "EPOCHS =  2#@param {type:\"integer\"} # Indicate the window length to parse the sequence into. Used in Data Parsing section\n",
        "\n",
        "# Indicate to clean data or not. Used in Data Cleaning section\n",
        "CLEAN = False #@param {type:\"boolean\"}\n",
        "DATA = \"Synthetic-Sendmail\" #@param {type:\"string\"}\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDDU0UkRplxK"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, accuracy_score, roc_curve, auc, recall_score, precision_score,plot_confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "#import nltk\n",
        "#nltk.download(\"popular\")\n",
        "\n",
        "## Tokenizing syscall sequences into n-grams of 6\n",
        "#from nltk.tokenize import word_tokenize\n",
        "#from nltk import ngrams"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load pre-processed data**"
      ],
      "metadata": {
        "id": "fSPiWe98mFwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "IMh4u3wrmFhm",
        "outputId": "19db8200-73f9-4003-9190-4c61d8ee3c95"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-73f1e076-7c03-4d93-a7d4-31d21be70b60\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-73f1e076-7c03-4d93-a7d4-31d21be70b60\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving test_clean.csv to test_clean.csv\n",
            "Saving test_unclean.csv to test_unclean.csv\n",
            "Saving train.csv to train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in data\n",
        "x_train = pd.read_csv('train.csv', header = 0)\n",
        "test_clean = pd.read_csv('test_clean.csv', header = 0)\n",
        "test_unclean = pd.read_csv('test_unclean.csv', header = 0)\n"
      ],
      "metadata": {
        "id": "3FaUODSsmFf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Separate data and label\n",
        "y_train = x_train['Label']\n",
        "x_train.drop(columns='Label', inplace=True)\n",
        "\n",
        "y_test_clean = test_clean['Label']\n",
        "test_clean.drop(columns = 'Label', inplace=True)\n",
        "\n",
        "y_test_unclean = test_unclean['Label']\n",
        "test_unclean.drop(columns ='Label', inplace=True)"
      ],
      "metadata": {
        "id": "eCDTcBAVVcZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_unclean"
      ],
      "metadata": {
        "id": "53vJsEi6mFdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train"
      ],
      "metadata": {
        "id": "FN2lx4dUHlM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "376udEA5GdPB"
      },
      "source": [
        " ## **Data Cleaning**\n",
        " Remove rows that exist in both normal and intrusion df"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzvxq_ViJscy"
      },
      "source": [
        "## Get % of duplicates in both datasets\n",
        "\n",
        "# Convert normal df to set, and intrusion df to set\n",
        "def clean_data(normal, intrusion):\n",
        "  normal_list = normal.values.tolist()\n",
        "  intrusion_list = intrusion.values.tolist()\n",
        "  normal_set = set(tuple(i) for i in normal_list)\n",
        "  intrusion_set = set(tuple(i) for i in intrusion_list)\n",
        "  print('List sz vs. Set sz of normal sequences: %d vs. %d'% (len(normal_list),len(normal_set)) )\n",
        "  print('List sz vs. Set sz of intrusion sequences: %d vs. %d'% (len(intrusion_list),len(intrusion_set)) )\n",
        "    \n",
        "  normal_dupplication = (len(normal_list) - len(normal_set)) /len(normal_list)*100 \n",
        "  intrusion_duplication = (len(intrusion_list)-len(intrusion_set))/len(intrusion_list) * 100\n",
        "\n",
        "  print('Duplication Rate in Normal Class: %.3f%%'% normal_dupplication )\n",
        "  print('Duplication Rate in Intrusion Class: %.3f%%'% intrusion_duplication) \n",
        " \n",
        "  c_intrusion = intrusion_set - normal_set \n",
        "  overlap_rate =  len(normal_set.intersection(intrusion_set)) / len(normal_set.union(intrusion_set)) * 100\n",
        "  print('Overlap rate: %.3f%%' % overlap_rate)\n",
        "  \n",
        "  #c_normal = normal_set - intrusion_set\n",
        "  if len(c_intrusion) == 0:\n",
        "    print(DATA+' No Duplication!')\n",
        "  if len(c_intrusion) > 0:\n",
        "    intrusion = pd.DataFrame(c_intrusion)\n",
        "  else:\n",
        "    intrusion = pd.DataFrame(intrusion_set)\n",
        "  #if len(c_normal) > 0:\n",
        "  #  normal = pd.DataFrame(c_normal)\n",
        "  #else:\n",
        "  normal = pd.DataFrame(normal_set)\n",
        "\n",
        "  print('After cleaning: \\nNormal sz:', len(normal), ' Intrusion sz:', len(c_intrusion) )\n",
        "  return normal, intrusion"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztHCbJZ3Lsxq"
      },
      "source": [
        "# If the CLEAN parameter at the top is checked, we'll train the model with clean data\n",
        "if CLEAN:\n",
        "  filt = y_train == 0\n",
        "  train_normal = x_train.loc[filt]\n",
        "  train_intrusion = x_train.loc[~filt]\n",
        "  normal, intrusion = clean_data(train_normal, train_intrusion) # clean normal and intrusion in Train \n",
        "\n",
        "  normal['Label'] = 0; intrusion['Label'] = 1\n",
        "  x_train = normal.append(intrusion, ignore_index = True)\n",
        "  x_train = x_train.sample(frac = 1)\n",
        "  y_train = x_train['Label']\n",
        "  x_train.drop(columns = 'Label', inplace = True)\n",
        "  print('x_train len after cleaning:', x_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bootstraping Data**"
      ],
      "metadata": {
        "id": "Q2NchpWVI3v7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filt = y_train == 0\n",
        "normal_train = x_train.loc[filt]\n",
        "normal_train\n",
        "\n",
        "intrusion_train = x_train.loc[~filt]\n",
        "intrusion_train"
      ],
      "metadata": {
        "id": "r_DQodRpIVKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bootstrap training data\n",
        "\n",
        "## Lived-name has more intrusion cases than normal cases (189 > 71) --> bootstrap normal cases only\n",
        "x_train['Label'] = y_train\n",
        "\n",
        "if len(intrusion_train) > len(normal_train):\n",
        "  x_train = x_train.iloc[intrusion_train.index].append(x_train.iloc[normal_train.index].sample(n = len(intrusion_train), replace=True), ignore_index=True) #upsampled normal data and add to train set\n",
        "else:\n",
        "  x_train = x_train.iloc[normal_train.index].append(x_train.iloc[intrusion_train.index].sample(n = len(normal_train), replace=True), ignore_index=True) #upsampled intrusion data and add to train set\n",
        "\n",
        "#x_train = x_train.append(x_train.sample(frac=1), ignore_index=True) # Bootstrap training data in case there is not enough data\n",
        "x_train = x_train.sample(frac= SZ) # Shuffle data with a SZ proportion\n",
        "x_train.reset_index(drop=True, inplace=True)\n",
        "y_train = x_train['Label']\n",
        "x_train.drop(columns='Label', inplace=True)\n",
        "x_train"
      ],
      "metadata": {
        "id": "yWVieVu6IaAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_clean"
      ],
      "metadata": {
        "id": "hsuzHnN6Nac-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_unclean"
      ],
      "metadata": {
        "id": "gPRDg5tbNc3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0UCA-ZuXHX_"
      },
      "source": [
        "\n",
        "# **Performance Measures**\n",
        "\n",
        "\n",
        "1.   Function calc_false_positive: Calculates FPR\n",
        "2.   Function print_performance: Formats printing performance metrics and ROC curve for each model\n",
        "3.   Function color_confusion_matrix: prints out a heatmap of confusion matrix in blue color scale\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XA1PZX4XBNLO"
      },
      "source": [
        "# This function calculate False Positive Rate given a confusion matrix\n",
        "def calc_false_positive (cmatrix):\n",
        "  specificity = cmatrix[0,0]/(cmatrix[0,0] + cmatrix[0,1])\n",
        "  return 1-specificity\n",
        "\n",
        "# This function prints performance metrics and ROC curve given the model name, true labels and predicted labels\n",
        "def print_performance( model_name, true_labels, pred_labels):\n",
        "  # rows are actual, columns are predicted\n",
        "  cmatrix = confusion_matrix(true_labels, pred_labels)\n",
        "  fpr = calc_false_positive(cmatrix)\n",
        "\n",
        "  print('Confusion Matrix: \\n',cmatrix)\n",
        "  print('\\nTesting Accuracy: %.2f'% metrics.accuracy_score(true_labels, pred_labels))\n",
        "  print('Precision:%.2f'%  metrics.precision_score(true_labels, pred_labels))\n",
        "  print('Recall: %.2f'% metrics.recall_score(true_labels, pred_labels))\n",
        "  print('False Positive Rate: %.2f'% fpr)\n",
        "  print('\\nClassification report:', classification_report(true_labels, pred_labels), sep='\\n')\n",
        "  print('AUC: %.2f'% roc_auc_score(true_labels, pred_labels))\n",
        "\n",
        "  false_positive_rate, recall, thresholds = roc_curve(true_labels, pred_labels)\n",
        "  roc_auc = auc(false_positive_rate, recall)\n",
        "  plt.figure()\n",
        "  if CLEAN: clean_status='Clean '\n",
        "  else: clean_status ='Overlapped and Duplicated '\n",
        "  plt.title( model_name+' ROC Curve on '+ clean_status + DATA + ' with Seq Len of '+ str(SEQ_WINDOW))\n",
        "  plt.plot(false_positive_rate, recall, 'b', label = 'AUC = %0.2f' %roc_auc)\n",
        "  plt.legend(loc='lower right')\n",
        "  plt.plot([0,1], [0,1], 'r--')\n",
        "  plt.xlim([0.0,1.0])\n",
        "  plt.ylim([0.0,1.1])\n",
        "  plt.ylabel('Recall')\n",
        "  plt.xlabel('False Positive Rate (1-Specificity)')\n",
        "  #plt.savefig(model_name+'-ROC.jpg')\n",
        "  plt.show()\n",
        "\n",
        "# Plot a heatmap of confusion matrix given the model name, a classifier model, testing data and the predicted label\n",
        "def color_confusion_matrix( model_name, model, x_test, y_test, y_predicted):\n",
        "  class_names = ['Normal', 'Intrusion']\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(6, 6))\n",
        "  plot_confusion_matrix(model, x_test, y_test, display_labels=class_names, \n",
        "                        values_format='d', ax = ax, cmap=plt.cm.Blues)\n",
        "  plt.title('Confusion Matrix of ' + str(model_name))\n",
        "  #plt.savefig(model_name+'-CM.jpg')\n",
        "  plt.show()\n",
        "\n",
        "  cmatrix = confusion_matrix(y_test, y_predicted)\n",
        "  print(cmatrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1SbGMBTCCcw"
      },
      "source": [
        "# Graphing overlaid ROC curves, where each one represents a model AUC score\n",
        "def graph_multi_ROC ():\n",
        "  # Set color for each model\n",
        "  colors = {'KM': 'lightcoral','LR': 'darkorange', 'SVM':'lime', 'NB': 'steelblue',\n",
        "            'NN': 'purple','DT': 'magenta','RF': 'deeppink','KNN': 'darkturquoise',\n",
        "            'BERT': 'darkred', 'GPT': 'blue'}\n",
        "  # Set marker for each model          \n",
        "  markers = {'KM':'1--','LR': 'v--', 'SVM': '^--', 'NN': '*--', 'DT': 'o--', 'RF': '+--', 'KNN': '.--', 'NB': 'x--', 'BERT':'<--', 'GPT': '>--'}\n",
        "  \n",
        "  plt.figure(figsize=(9,6))\n",
        "  try:\n",
        "    plt.plot(KM_test.get('fpr'), KM_test.get('tpr'), markers.get('KM'), color=colors.get('KM'),  label=\"KM - AUC=\" + str(KM_test.get('auc').round(3)))\n",
        "    plt.plot(LR_test.get('fpr'), LR_test.get('tpr'), markers.get('LR'), color=colors.get('LR'),  label=\"LR - AUC=\" + str(LR_test.get('auc').round(3)))\n",
        "    plt.plot(SVM_test.get('fpr'),SVM_test.get('tpr'),markers.get('SVM'),color=colors.get('SVM'), label=\"SVM - AUC=\"+ str(SVM_test.get('auc').round(3)))\n",
        "    plt.plot(NN_test.get('fpr'), NN_test.get('tpr'), markers.get('NN'), color=colors.get('NN'),  label=\"NN - AUC=\" + str(NN_test.get('auc').round(3)))\n",
        "    plt.plot(DT_test.get('fpr'), DT_test.get('tpr'), markers.get('DT'), color=colors.get('DT'),  label=\"DT - AUC=\" + str(DT_test.get('auc').round(3)))\n",
        "    plt.plot(RF_test.get('fpr'), RF_test.get('tpr'), markers.get('RF'), color=colors.get('RF'),  label=\"RF - AUC=\" + str(RF_test.get('auc').round(3)))\n",
        "    plt.plot(KNN_test.get('fpr'),KNN_test.get('tpr'),markers.get('KNN'),color=colors.get('KNN'), label=\"KNN - AUC=\"+ str(KNN_test.get('auc').round(3)))\n",
        "    plt.plot(NB_test.get('fpr'), NB_test.get('tpr'), markers.get('NB'), color=colors.get('NB'),  label=\"NB - AUC=\" + str(NB_test.get('auc').round(3)))\n",
        "  except:\n",
        "    print('only performances of BERT and GPT are available')\n",
        "\n",
        "  plt.plot(BERT_test.get('fpr'),BERT_test.get('tpr'), markers.get('BERT'), color=colors.get('BERT'),  label=\"BERT - AUC=\"+ str(BERT_test.get('auc').round(3)))\n",
        "  plt.plot(GPT_test.get('fpr'),GPT_test.get('tpr'), markers.get('GPT'), color=colors.get('GPT'),  label=\"GPT-2 - AUC=\"+ str(GPT_test.get('auc').round(3)))\n",
        "\n",
        "  plt.plot([0,1], [0,1], 'k--', label='Random Chances')\n",
        "  plt.xlim([0.0,1.0])\n",
        "  plt.ylim([0.0,1.02])\n",
        "  plt.ylabel('Recall')\n",
        "  plt.xlabel('False Positive Rate (1-Specificity)')\n",
        "  plt.legend(loc='lower right') \n",
        "  plt.title( 'Testing ROCs on ' + DATA)\n",
        "  #plt.savefig(DATA_I+'-'+train_or_test+'.jpg', dpi = 80)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ-JMcBqmnN9"
      },
      "source": [
        "# **BERT**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7k6L-u3Lmmpb"
      },
      "source": [
        "!pip install pytorch_pretrained_bert pytorch-nlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsHMmMB76B9W"
      },
      "source": [
        "from pytorch_pretrained_bert import BertModel\n",
        "from torch import nn\n",
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.optim import Adam\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_NgiJJQm6Xx"
      },
      "source": [
        "**Prepare for Train and test data for BERT**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDMTse2Jm0Bc"
      },
      "source": [
        "train_texts = []\n",
        "for i in range(x_train.shape[0]):\n",
        "     train_texts.append(\" \".join(np.array(x_train.iloc[i,:]).astype(str)))\n",
        "train_texts = tuple(train_texts) \n",
        "\n",
        "clean_test_texts = []\n",
        "for i in range(test_clean.shape[0]):\n",
        "    clean_test_texts.append(\" \".join(np.array(test_clean.iloc[i,:]).astype(str)))\n",
        "clean_test_texts = tuple(clean_test_texts) \n",
        "\n",
        "unclean_test_texts = []\n",
        "for i in range(test_unclean.shape[0]):\n",
        "     unclean_test_texts.append(\" \".join(np.array(test_unclean.iloc[i,:]).astype(str)))\n",
        "unclean_test_texts = tuple(unclean_test_texts) \n",
        "\n",
        "\n",
        "train_labels        = tuple(y_train.tolist())\n",
        "clean_test_labels   = tuple(y_test_clean.tolist())\n",
        "unclean_test_labels = tuple(y_test_unclean.tolist())\n",
        "\n",
        "len(train_texts), len(train_labels), len(clean_test_texts), len(clean_test_labels), len(unclean_test_texts), len(unclean_test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gf2Lbwylm_ia"
      },
      "source": [
        "# Tokenizer \n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "# Example\n",
        "tokenizer.tokenize(train_texts[8])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2B9WKzfendjw"
      },
      "source": [
        "# Prepare labels\n",
        "# True if intrusion or False if normal\n",
        "# Prepare labels\n",
        "# True if intrusion or False if normal\n",
        "train_y = np.array(train_labels) == 1\n",
        "test_clean_y = np.array(clean_test_labels) == 1\n",
        "test_unclean_y = np.array(unclean_test_labels) == 1\n",
        "\n",
        "train_y.shape, test_clean_y.shape, test_unclean_y.shape, np.mean(train_y), np.mean(test_clean_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dp_XAarnZqN"
      },
      "source": [
        "# Convert to tokens using tokenizer\n",
        "train_tokens        = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:SEQ_WINDOW] + ['[SEP]'], train_texts))\n",
        "clean_test_tokens   = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:SEQ_WINDOW] + ['[SEP]'], clean_test_texts))\n",
        "unclean_test_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:SEQ_WINDOW] + ['[SEP]'], unclean_test_texts))\n",
        "\n",
        "print('Number of Training Sequences:',len(train_tokens), '\\nNumber of Clean Testing Sequences:', len(clean_test_tokens), '\\nNumber of Unclean Testing Sequences:', len(unclean_test_tokens) )       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MH_bWrznbYv"
      },
      "source": [
        "# Following is to convert List of words to list of numbers. (Words are replaced by their index in dictionar)\n",
        "train_tokens_ids         = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, train_tokens)), maxlen= SEQ_WINDOW, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
        "clean_test_tokens_ids    = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, clean_test_tokens)),  maxlen= SEQ_WINDOW, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
        "unclean_test_tokens_ids  = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, unclean_test_tokens)),  maxlen= SEQ_WINDOW, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
        "\n",
        "train_tokens_ids.shape, clean_test_tokens_ids.shape, unclean_test_tokens_ids.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1q-b-LKnfbH"
      },
      "source": [
        "# To mask the paddings\n",
        "train_masks        = [[float(i > 0) for i in ii] for ii in train_tokens_ids]\n",
        "clean_test_masks   = [[float(i > 0) for i in ii] for ii in clean_test_tokens_ids]\n",
        "unclean_test_masks = [[float(i > 0) for i in ii] for ii in unclean_test_tokens_ids]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tZ3gKOtng_v"
      },
      "source": [
        "# Define BERT model\n",
        "class BertBinaryClassifier(nn.Module):\n",
        "    def __init__(self, dropout=0.1):\n",
        "        super(BertBinaryClassifier, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear = nn.Linear(768, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, tokens, masks=None):\n",
        "        # First Layer\n",
        "        _, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)\n",
        "\n",
        "        dropout_output = self.dropout(pooled_output)\n",
        "\n",
        "        linear_output = self.linear(dropout_output)\n",
        "        \n",
        "        # output layer\n",
        "        proba = self.sigmoid(linear_output)\n",
        "        \n",
        "        return proba\n",
        "      \n",
        "    def train_m(self,x,y,train_mask,epochs,batchsize):\n",
        "      train_tokens_tensor = torch.tensor(x)\n",
        "      train_y_tensor = torch.tensor(y.reshape(-1, 1)).float()\n",
        "      train_masks_tensor = torch.tensor(train_mask)\n",
        "\n",
        "      train_dataset = TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)\n",
        "      train_sampler = RandomSampler(train_dataset)\n",
        "      train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batchsize) \n",
        "\n",
        "\n",
        "      param_optimizer = list(self.sigmoid.named_parameters()) \n",
        "      optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
        "      optimizer = Adam(self.bert.parameters(), lr=2e-5)\n",
        "      for epoch_num in range(epochs):\n",
        "          self.train() # Training Flag\n",
        "          train_loss = 0\n",
        "          for step_num, batch_data in enumerate(train_dataloader):\n",
        "              \n",
        "              # Load batch on device memory\n",
        "              token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
        "\n",
        "              # Get the output of the model for provided input\n",
        "              logits = self(token_ids, masks)\n",
        "              \n",
        "              # Loss function\n",
        "              loss_func = nn.BCELoss()\n",
        "\n",
        "              # Calculate Loss\n",
        "              batch_loss = loss_func(logits, labels)\n",
        "              train_loss += batch_loss.item()\n",
        "              \n",
        "              # backpropagate the error\n",
        "              self.zero_grad()\n",
        "              batch_loss.backward()\n",
        "              \n",
        "              # Update the Weights of the Model\n",
        "              clip_grad_norm_(parameters=self.parameters(), max_norm=1.0)\n",
        "              optimizer.step()\n",
        "              \n",
        "              clear_output(wait=True)\n",
        "              print('Epoch: ', epoch_num + 1)\n",
        "              print(\"\\r\" + \"{0}/{1} loss: {2} \".format(step_num, len(train_labels) / batchsize, train_loss / (step_num + 1)))        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MA0ADfkpnjgv"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CZERb2Onk_v"
      },
      "source": [
        "bert_clf = BertBinaryClassifier()\n",
        "bert_clf = bert_clf.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjM08A0OoGro"
      },
      "source": [
        "**Fine Tune BERT**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUS_cifIn1_Y"
      },
      "source": [
        "# Train BERT NLP\n",
        "bert_clf.train_m(train_tokens_ids,train_y,train_masks, EPOCHS, BATCH_SZ)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUTlJLsooNrq"
      },
      "source": [
        "**Evaluate on Testing Set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ly5Mhn9SoKxw"
      },
      "source": [
        "# Convert token ids to tensor \n",
        "clean_test_tokens_tensor = torch.tensor(clean_test_tokens_ids)\n",
        "unclean_test_tokens_tensor = torch.tensor(unclean_test_tokens_ids)\n",
        "\n",
        "# Convert labels to tensors\n",
        "clean_test_y_tensor = torch.tensor(test_clean_y.reshape(-1, 1)).float()\n",
        "unclean_test_y_tensor = torch.tensor(test_unclean_y.reshape(-1, 1)).float()\n",
        "\n",
        "# Convert to tensro for maks\n",
        "clean_test_masks_tensor   = torch.tensor(clean_test_masks)\n",
        "unclean_test_masks_tensor = torch.tensor(unclean_test_masks)\n",
        "\n",
        "# Load Token, token mask and label into Dataloader\n",
        "clean_test_dataset   = TensorDataset(clean_test_tokens_tensor, clean_test_masks_tensor, clean_test_y_tensor)\n",
        "unclean_test_dataset = TensorDataset(unclean_test_tokens_tensor, unclean_test_masks_tensor, unclean_test_y_tensor)\n",
        "\n",
        "# Define sampler\n",
        "clean_test_sampler = SequentialSampler(clean_test_dataset)\n",
        "unclean_test_sampler = SequentialSampler(unclean_test_dataset)\n",
        "\n",
        "# Defile test data loader\n",
        "clean_test_dataloader   = DataLoader(clean_test_dataset,   sampler = clean_test_sampler,   batch_size=128)\n",
        "unclean_test_dataloader = DataLoader(unclean_test_dataset, sampler = unclean_test_sampler, batch_size=128)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_Bert(dataloader):\n",
        "  bert_clf.eval() # Define eval\n",
        "  bert_predicted = [] # To Store predicted result\n",
        "  all_logits = [] # Actual output that is between 0 to 1 is stored here\n",
        "  with torch.no_grad():\n",
        "      for step_num, batch_data in enumerate(dataloader):\n",
        "\n",
        "          # Load the batch on gpu memory\n",
        "          token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
        "\n",
        "          # Calculate ouput of bert\n",
        "          logits = bert_clf(token_ids, masks)\n",
        "\n",
        "          # Get the numpy logits\n",
        "          numpy_logits = logits.cpu().detach().numpy()  # Detach from the GPU memory\n",
        "          \n",
        "          # Using the threshold find binary \n",
        "          bert_predicted += list(numpy_logits[:, 0] > 0.5)  # Threshold conversion\n",
        "          all_logits += list(numpy_logits[:, 0])\n",
        "  return bert_predicted\n",
        "\n",
        "def get_Bert_performance(dataloader, label):\n",
        "  bert_predicted = evaluate_Bert(dataloader)\n",
        "  print_performance('BERT', label, bert_predicted)\n",
        "\n",
        "  # Recording TPR and FPR for the TESTING-ROC curves\n",
        "  performance = {}\n",
        "  performance['fpr'], performance['tpr'], thresh = roc_curve(label, bert_predicted)\n",
        "  performance['auc'] = roc_auc_score(label, bert_predicted)\n",
        "  return performance\n",
        "\n"
      ],
      "metadata": {
        "id": "_Lu3SkuxWTv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('----------------------------Evaluating BERT with Clean Data----------------------------')\n",
        "BERT_clean_perf = get_Bert_performance(clean_test_dataloader, test_clean_y)"
      ],
      "metadata": {
        "id": "5-kZjoPnWXAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('----------------------------Evaluating BERT with Unclean Data----------------------------')\n",
        "BERT_unclean_perf = get_Bert_performance(unclean_test_dataloader, test_unclean_y)\n",
        "write_to_file(BERT_clean_perf,   'BERT', 'clean')\n",
        "write_to_file(BERT_unclean_perf, 'BERT', 'unclean')"
      ],
      "metadata": {
        "id": "XK7rIyecWZPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSuD_5c43X10"
      },
      "source": [
        "# **GPT-2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HueSQ1KsukV"
      },
      "source": [
        "!pip install transformers\n",
        "\n",
        "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-gD9nyo4S1D"
      },
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('microsoft/DialoGPT-small')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkYOrTFA4YUN"
      },
      "source": [
        "# Padding sequences from the right to a max length of 20\n",
        "tokenizer.padding_side = \"right\"\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "train_tokens        = tokenizer(train_texts,      return_tensors='pt',  truncation=True,  padding=True,  max_length = SEQ_WINDOW)\n",
        "clean_test_tokens   = tokenizer(clean_test_texts, return_tensors='pt',  truncation=True,  padding=True,  max_length = SEQ_WINDOW)\n",
        "unclean_test_tokens = tokenizer(unclean_test_texts,return_tensors='pt', truncation=True,  padding=True,  max_length = SEQ_WINDOW)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHRofoti4g24"
      },
      "source": [
        "# Following is to convert List of words to list of numbers. (Words are replaced by their index in dictionar)\n",
        "\n",
        "train_tokens_ids        = train_tokens.input_ids\n",
        "clean_test_tokens_ids   = clean_test_tokens.input_ids\n",
        "unclean_test_tokens_ids = unclean_test_tokens.input_ids\n",
        "\n",
        "train_tokens_ids.shape, clean_test_tokens_ids.shape, unclean_test_tokens_ids.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udvDwEdz4haO"
      },
      "source": [
        "train_masks        = train_tokens.attention_mask\n",
        "clean_test_masks   = clean_test_tokens.attention_mask\n",
        "unclean_test_masks = unclean_test_tokens.attention_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23qklJA_6Q2L"
      },
      "source": [
        "**Create GPT-2 Classifer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LiNms1g6PqS"
      },
      "source": [
        "class GTP2BinaryClassifier(nn.Module):\n",
        "    def __init__(self, dropout=0.1):\n",
        "        super(GTP2BinaryClassifier, self).__init__()\n",
        "        self.gtp2 = GPT2ForSequenceClassification.from_pretrained('microsoft/DialoGPT-small')\n",
        "      \n",
        "    def train_m(self,x,y,train_mask,epochs,batchsize):\n",
        "      train_tokens_tensor = torch.tensor(x)\n",
        "      train_y_tensor = torch.tensor(y.reshape(-1, 1)).long()\n",
        "      train_masks_tensor = torch.tensor(train_mask)\n",
        "\n",
        "      train_dataset = TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)\n",
        "      train_sampler = RandomSampler(train_dataset)\n",
        "      train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batchsize) \n",
        "\n",
        "\n",
        "      # param_optimizer = list(self.gtp2.parameters()) \n",
        "      # optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
        "      optimizer = Adam(self.gtp2.parameters(), lr=5e-5)\n",
        "      for epoch_num in range(epochs):\n",
        "          self.gtp2.train() # Training Flag\n",
        "          train_loss = 0\n",
        "          for step_num, batch_data in enumerate(train_dataloader):\n",
        "              \n",
        "              # Load batch on device memory\n",
        "              token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
        "              self.zero_grad()\n",
        "\n",
        "              # Get the output of the model for provided input\n",
        "              outputs = self.gtp2(token_ids,attention_mask=masks,labels=labels)\n",
        "              loss, logits = outputs[:2]\n",
        "              # logits = self(token_ids, masks)\n",
        "              \n",
        "              # Total Loss\n",
        "              train_loss += loss.item()\n",
        "              \n",
        "              # Backward pass the loss\n",
        "              loss.backward()\n",
        "              torch.nn.utils.clip_grad_norm_(self.gtp2.parameters(), 1.0)\n",
        "              \n",
        "              optimizer.step()\n",
        "              logits = logits.detach().cpu().numpy()\n",
        "\n",
        "              clear_output(wait=True)\n",
        "        \n",
        "              print('Epoch: ', epoch_num + 1)\n",
        "              print(\"\\r\" + \"{0}/{1} loss: {2} \".format(step_num, len(train_labels) / batchsize, train_loss / (step_num + 1)))\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPZ0MZU06Xxu"
      },
      "source": [
        "gtp_clf = GTP2BinaryClassifier()\n",
        "gtp_clf = gtp_clf.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uc6BBhJ_6gQL"
      },
      "source": [
        "# Configure the Padding token id\n",
        "gtp_clf.gtp2.config.pad_token_id = tokenizer.eos_token_id\n",
        "gtp_clf.train_m(train_tokens_ids,train_y,train_masks, EPOCHS, BATCH_SZ)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIQXu7cL6n1g"
      },
      "source": [
        "**Evaluate on Testing Set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpBPha-z6gyA"
      },
      "source": [
        "clean_test_tokens_tensor = torch.tensor(clean_test_tokens_ids)\n",
        "unclean_test_tokens_tensor = torch.tensor(unclean_test_tokens_ids)\n",
        "clean_test_y_tensor   = torch.tensor(test_clean_y.reshape(-1, 1)).long()\n",
        "unclean_test_y_tensor = torch.tensor(test_unclean_y.reshape(-1, 1)).long()\n",
        "\n",
        "clean_test_masks_tensor   = torch.tensor(clean_test_masks)\n",
        "unclean_test_masks_tensor = torch.tensor(unclean_test_masks)\n",
        "\n",
        "\n",
        "clean_test_dataset   = TensorDataset(clean_test_tokens_tensor, clean_test_masks_tensor, clean_test_y_tensor)\n",
        "unclean_test_dataset = TensorDataset(unclean_test_tokens_tensor, unclean_test_masks_tensor, unclean_test_y_tensor)\n",
        "\n",
        "clean_test_sampler = SequentialSampler(clean_test_dataset)\n",
        "unclean_test_sampler = SequentialSampler(unclean_test_dataset)\n",
        "\n",
        "clean_test_dataloader = DataLoader(clean_test_dataset,     sampler= clean_test_sampler,   batch_size=128)\n",
        "unclean_test_dataloader = DataLoader(unclean_test_dataset, sampler= unclean_test_sampler, batch_size=128)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Model\n",
        "def evaluate_GPT(dataloader):\n",
        "  gtp_clf.eval() # Define eval\n",
        "  gpt_predicted = [] # Store Result\n",
        "  with torch.no_grad():\n",
        "      for step_num, batch_data in enumerate(dataloader):\n",
        "\n",
        "          token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
        "\n",
        "          # ----------------------------------------------------------------\n",
        "          outputs = gtp_clf.gtp2(token_ids,attention_mask=masks,labels=labels)\n",
        "          loss, logits = outputs[:2]\n",
        "          numpy_logits = logits.detach().cpu().numpy()\n",
        "          # ----------------------------------------------------------------\n",
        "          gpt_predicted +=list(numpy_logits.argmax(axis=-1).flatten().tolist())\n",
        "  return gpt_predicted\n",
        "\n",
        "\n",
        "def get_GPT_performance(dataloader, label):\n",
        "  gpt_predicted = evaluate_GPT(dataloader)\n",
        "  print_performance('GPT', label, gpt_predicted)\n",
        "\n",
        "  # Recording TPR and FPR for the TESTING-ROC curves\n",
        "  performance = {}\n",
        "  performance['fpr'], performance['tpr'], thresh = roc_curve(label, gpt_predicted)\n",
        "  performance['auc'] = roc_auc_score(label, gpt_predicted)\n",
        "  return performance"
      ],
      "metadata": {
        "id": "P9Az4S2KXKzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('----------------------------Evaluating GPT with Clean Data----------------------------')\n",
        "GPT_clean_perf = get_GPT_performance(clean_test_dataloader, test_clean_y)"
      ],
      "metadata": {
        "id": "K1oq6bS3XMcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('----------------------------Evaluating GPT with Unclean Data----------------------------')\n",
        "GPT_unclean_perf = get_GPT_performance(unclean_test_dataloader, test_unclean_y)"
      ],
      "metadata": {
        "id": "0iNlJ50cXOjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJgQwUXi62OA"
      },
      "source": [
        "# Ploting the overlaid ROC curves on testing results:\n",
        "#graph_multi_ROC()\n",
        "\n",
        "\n",
        "def plot_ROC_Clean_Unclean(clean, unclean, model_name):\n",
        "  colors = {'unclean': 'lightcoral','clean': 'blue'}\n",
        "\n",
        "  plt.plot(clean.get('fpr'), clean.get('tpr'), color=colors.get('clean'),  label= \"AUC on Clean Data =\" + str( round(clean.get('auc'), 3) ) )   \n",
        "  plt.plot(unclean.get('fpr'), unclean.get('tpr'), color=colors.get('unclean'),  label= \"AUC on Unclean Data =\" + str( round(unclean.get('auc'), 3) ) )   \n",
        "  plt.title(model_name +' Performance on Clean and Unclean Data')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "plot_ROC_Clean_Unclean(BERT_clean_perf, BERT_unclean_perf, 'BERT')\n",
        "plot_ROC_Clean_Unclean(GPT_clean_perf, GPT_unclean_perf, 'GPT')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-KVm3qLDNsk"
      },
      "source": [
        "# **Write performance measures to file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CIEcZ4ke9k1"
      },
      "source": [
        "# Save performance measure dict of each model to a file\n",
        "def write_to_file (varname, model_name, clean):\n",
        "  clean_status = 'clean' if CLEAN else 'unclean'\n",
        "  filename = DATA +'-'+ str(SEQ_WINDOW) +'-'+ clean_status + \"-model.txt\"\n",
        "  file = open(filename, \"a\")\n",
        "  str_dictionary = repr(varname)\n",
        "  file.write(\"{}_test_{} = \".format(model_name, clean) + str_dictionary + \"\\n\")\n",
        "  file.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uusevnMCe9kO"
      },
      "source": [
        "write_to_file(GPT_clean_perf,   'GPT', 'clean')\n",
        "write_to_file(GPT_unclean_perf, 'GPT', 'unclean')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "print(BERT_test)\n",
        "\n",
        "print('\\n',GPT_test)"
      ],
      "metadata": {
        "id": "yE_RchiQ68eJ"
      }
    }
  ]
}